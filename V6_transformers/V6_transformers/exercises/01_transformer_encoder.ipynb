{"metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}}, "nbformat": 4, "nbformat_minor": 2, "cells": [{"cell_type": "markdown", "metadata": {}, "source": "# Transformer Model - Exercises"}, {"cell_type": "markdown", "metadata": {}, "source": "In this tutorial we will explore the Transformer architecture which is the fundamental building block of today's Large Language Models. A focus will be set on the transformer encoder including the self-attention layer. We will implement a sentiment classification model which predicts the binary sentiment of given movie reviews: positive or negative.\n"}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2025-11-28T13:27:08.583293Z", "start_time": "2025-11-28T13:27:03.332736Z"}}, "outputs": [], "source": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\nimport torch.optim as optim\nimport numpy as np\nimport tqdm as tqdm\nimport matplotlib.pyplot as plt\nimport pickle\nfrom typing import List, Dict"}, {"cell_type": "markdown", "metadata": {}, "source": "#### Data Preparation"}, {"cell_type": "markdown", "metadata": {}, "source": "We start of by loading the movie reviews as well as the corresponding sentiment labels. The label ```y=0``` corresponds to negative sentiment, ```y=1``` corresponds to positive sentiment.\n\nInvestigate the printed samples to familiarize yourself with the data."}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2025-11-28T13:27:08.624460Z", "start_time": "2025-11-28T13:27:08.585338Z"}}, "outputs": [], "source": "with open(\"data/movie_reviews.pkl\", \"rb\") as fp:  \n    texts = pickle.load(fp)\nwith open(\"data/movie_labels.pkl\", \"rb\") as fp:\n    labels = pickle.load(fp)\n\nfor text, label in zip(texts[:5], labels):\n    print(\"Review:\", text, \"Sentiment:\", label.item())"}, {"cell_type": "markdown", "metadata": {}, "source": "##### Tokenization\nIn the first processing step we want to tokenize the input sequences. We choose to use simple tokenization and just treat each individual word (seperated by a blank space) as a token. \nImplement the function ```tokenize_text``` which receives a list of movie reviews. \nEach review of type `str` is transformed into a list containing the seperate tokens of the sequence (i.e. a list with elements of type `str`). \n\nFurther, add the classification token ```<cls>``` to the beginning of each token list.\nThis token will later be used as an indicator that a classification based on the subsequent tokens is queried and marks the position where the corresponding output (class prediction) will be computed (see below)."}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2025-11-28T13:27:08.629338Z", "start_time": "2025-11-28T13:27:08.625468Z"}}, "outputs": [], "source": "def tokenize_text(texts: List[str]) -> List[List[str]]:\n    tokenized_texts = []\n\n    ### YOUR SOLUTION HERE\n    ### END OF SOLUTION"}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2025-11-28T13:27:08.638510Z", "start_time": "2025-11-28T13:27:08.630756Z"}}, "outputs": [], "source": "texts_tokenized = tokenize_text(texts)\nassert len(texts_tokenized) == len(texts)"}, {"cell_type": "markdown", "metadata": {}, "source": "Next, we create a dictionary to encode the tokens to numeric values, which can be processed by the transformer model. \nTo do so, we map each individual token to a unique index, and, at the same time, assemble the inverse mapping for decoding. \nImplement the function ```create_dictionaries``` which takes the list of tokenized input sequences and creates both, the ```encoding_dict``` as well as the ```decoding_dict``` which will be used to convert tokens to indices and vice versa. \nThe classification token ```<cls>``` is assigned the index ```0```. \nAlso add a padding token (`pad`) and a token for unknown words (```<unk>```) to the dictionaries and assign the highest indices to them.\nThe padding token will be used to fill shorter sequences to identical lengths, which is necessary as the model requires all input sequences to be of identical lengths."}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2025-11-28T13:27:08.647701Z", "start_time": "2025-11-28T13:27:08.640667Z"}}, "outputs": [], "source": "def create_dictionaries(texts_tokenized):\n    encoding_dict = {}\n    decoding_dict = {}\n    encoding_dict[\"<cls>\"] = 0\n    decoding_dict[0] = \"<cls>\"\n    idx = 1\n\n    ### YOUR SOLUTION HERE\n    ### END OF SOLUTION\n    return encoding_dict, decoding_dict"}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2025-11-28T13:27:08.653944Z", "start_time": "2025-11-28T13:27:08.648973Z"}}, "outputs": [], "source": "encoding_dict, decoding_dict = create_dictionaries(texts_tokenized)\nassert \"film\" == decoding_dict[encoding_dict[\"film\"]]\nassert \"music\" == decoding_dict[encoding_dict[\"music\"]]"}, {"cell_type": "markdown", "metadata": {}, "source": "The encoding dictionary can be used to convert the tokenized training text to numerical values, which can be used as input to our language model.\n\nImplement the function `encode_token_sequence(tokenized_sequence: List[str], encoding_dict:Dict[str, int], default_token = \"<unk>\")` which encodes a sequence of tokens based on the given dictionary.\nWhenever the sequence contains a token which is not contained in the encoding dictionary, use the default token instead."}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2025-11-28T13:27:08.661804Z", "start_time": "2025-11-28T13:27:08.654953Z"}}, "outputs": [], "source": "def encode_token_sequence(tokenized_sequence: List[str], encoding_dict: Dict[str, int], default_token = \"<unk>\", max_length=8):\n    # query default token encoding once\n    default_token_encoded = encoding_dict[default_token]\n    encoded_sequence = []\n\n    ### YOUR SOLUTION HERE\n    ### END OF SOLUTION\n    return encoded_sequence"}, {"cell_type": "markdown", "metadata": {}, "source": "To convert a given sequence of indices back to tokenized text, implement the function ```decode_sequence(encoded_sequence: List[int], decoding_dict: Dict[int, str])``` which decodes a sequence of indices based on the given ```decoding_dict```."}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2025-11-28T13:27:08.667865Z", "start_time": "2025-11-28T13:27:08.662820Z"}}, "outputs": [], "source": "def decode_sequence(encoded_sequence: List[int], decoding_dict: Dict[int, str]):\n    ### YOUR SOLUTION HERE\n    ### END OF SOLUTION\n    return decoded_sequence"}, {"cell_type": "markdown", "metadata": {}, "source": "We use ```encode_token_sequence``` to convert the tokenized input sequences ```texts_tokenized``` to sequences of indices."}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2025-11-28T13:27:08.674880Z", "start_time": "2025-11-28T13:27:08.669245Z"}}, "outputs": [], "source": "encoded_texts = []\nfor tokenized_sequence in texts_tokenized:\n    encoded = encode_token_sequence(tokenized_sequence, encoding_dict)\n    encoded_texts.append(encoded)"}, {"cell_type": "markdown", "metadata": {}, "source": "#### Self-Attention Implementation\n##### Scaled dot-product attention\nAfter having performed the essential data pre-processing we will now implement the self-attention layer, which is the heart of both transformer encoder and transformer decoder. \nTherefore, we start off by defining the function ```scaled_dot_product_attention``` which receives ```query```, ```key``` and ```value``` tensors and returns the weighted sum of the ```value``` tensors according to the scaled dot-product formulation introduced in the lecture:\n$$\\text{attn}(Q,K,V)=\\text{softmax} \\bigg( \\dfrac{QK^T}{\\sqrt{d_k}} \\bigg) V$$\n"}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2025-11-28T13:27:08.681002Z", "start_time": "2025-11-28T13:27:08.675902Z"}}, "outputs": [], "source": "def scaled_dot_product_attention(query: torch.Tensor, key: torch.Tensor, value: torch.Tensor):\n    ### YOUR SOLUTION HERE\n    ### END OF SOLUTION\n    return weighted_values, scores"}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2025-11-28T13:27:08.708119Z", "start_time": "2025-11-28T13:27:08.682022Z"}}, "outputs": [], "source": "sequence_length = 4\ndk = 128 # dimension of key tensors = dimension of query tensors\ndv = 64 # dimension of value tensors\nquery = torch.randn(sequence_length, dk)\nkey = torch.randn(sequence_length, dk)\nvalue = torch.randn(sequence_length, dv)\n\nweighted_values, scores = scaled_dot_product_attention(query, key, value)\nassert weighted_values.size(-1) == dv"}, {"cell_type": "markdown", "metadata": {}, "source": "##### SelfAttentionLayer\nWe continue to use the previously defined function ```scaled_dot_product_attention``` to implement a single layer of the Transformer Encoder. We define the class ```TransformerEncoderLayer``` which inherits from ```torch.nn.Module```. In the ```__init__``` method we first initialize the learnable weight matrices ```self.wq```, ```self.wk``` and ```self.wv``` for computing ```key```, ```query``` and ```value``` vectors in the ```forward``` method.\n\nTake a moment to reconsider that this projection of keys, queries and values can in fact be done using PyTorch's linear layer (as the computations of a neural network layer without activation function are basically a multiplication of the input vector by the layer's parameter-matrix).\n\n\n\n<img src=\"data/encoder_layer.png\" \n        alt=\"Picture\" \n        style=\"display: block; margin: 0 auto\" />"}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2025-11-28T13:27:08.713668Z", "start_time": "2025-11-28T13:27:08.709127Z"}}, "outputs": [], "source": "class TransformerEncoderLayer(torch.nn.Module):\n    def __init__(self, dmodel: int, dk: int, dv: int):\n        super().__init__()\n        # initialize key, query and value weight matrices for self-attention layer\n        ### YOUR SOLUTION HERE\n        ### END OF SOLUTION\n\n        # initialize layer norm and feed-forward network\n        self.layer_norm1 = nn.LayerNorm(dmodel)\n        self.linear1 = nn.Linear(dmodel, 1024)\n        self.linear2 = nn.Linear(1024, dmodel)\n        self.layer_norm2 = nn.LayerNorm(dmodel)\n\n    def forward(self, x: torch.Tensor):\n        ### YOUR SOLUTION HERE\n        # compute self-attention\n        # add and normalize\n        # feed-forward network\n        # add and normalize\n        ### END OF SOLUTION\n        return z, scores"}, {"cell_type": "markdown", "metadata": {}, "source": "Before using the ```TransformerEncoderLayer``` to implement the final model for the sentiment prediction we have to make sure that the model is able to reason about the positions of the individual tokens in the input sequence. In the lecture we introduced the positional encodings which are added to the token embeddings prior to being passed to the first encoder layer. These encodings are usually following fixed patterns (e.g. sine or cosine functions) which are depending on the respective position in the input sequence. By adding them to the token embeddings the model is able to learn about the absolute or relative positions of the individual tokens in the sequence. In this excercise we implement a positional encoding which follows a sine/cosine pattern. \nWe first initialize the empty matrix $P$ (called ```positional_encoding``` in the code below) of size ```(max_seq_length)```. Next, we start filling the matrix from the first row up to the ```max_length``` row:\n\n$$P(k, 2i) = \\sin(\\dfrac{k}{n^{2i/d}})$$\n$$P(k, 2i + 1) = \\cos(\\dfrac{k}{n^{2i/d}})$$\n\n$$ d: \\text{encoder model dimension}$$\n$$ k: \\text{position of a token in the input sequence} $$\n$$ n: \\text{user defined scaler}  $$\n$$ i: \\text{mapping to column indices} \\quad (0 \\leq i < d/2)$$"}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2025-11-28T13:27:08.741089Z", "start_time": "2025-11-28T13:27:08.714686Z"}}, "outputs": [], "source": "torch.manual_seed(14)\ntorch.cuda.manual_seed(14)\ndmodel = 128\ndk = 128\ndv = 128"}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2025-11-28T13:27:08.747550Z", "start_time": "2025-11-28T13:27:08.742101Z"}}, "outputs": [], "source": "def generate_positional_encoding(max_seq_length: int, d: int, n: int) -> torch.Tensor:\n    ### YOUR SOLUTION HERE\n    ### YOUR SOLUTION HERE\n# maximum sequence length"}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2025-11-28T13:27:08.967818Z", "start_time": "2025-11-28T13:27:08.748569Z"}}, "outputs": [], "source": "fig, axes = plt.subplots(1, 1, figsize=(24, 24))\naxes.imshow(encodings)"}, {"cell_type": "markdown", "metadata": {}, "source": "With the ```TransformerEncoderLayer``` and the positional encodings implemented we are now ready to set up the entire model for predicting the sentiment of given movie reviews. In the ```__init__``` method of our ```SentimentPredictionModel``` we initialize two instances ```self.encoder_layer1``` and ```self.encoder_layer2``` of the ```TransformerEncoderLayer```. In the ```forward``` method we first sum the positional encodings to the token embeddings. These embeddings are then passed through both encoder layers. \n\nFor our goal of sentiment classification, we will train the model's output in position of the classification token ```<cls>``` (which is at the first position in our input sequence) to contain the class prediction for the complete sequence. \nThrough the self-attention layer, the encoding of the ```<cls>``` should be able to attend to all other tokens of the input sequence and can therefore access all the relevant information of the sequence to classify. \nThe final prediction head computes the class logits for both negative and positive sentiment based on that model output for the classification indicator `<cls>`.\nThese logits are returned for loss computation.\n\n(The model output for all other positions in the sequence are not relevant in this case.\nDuring training, no loss is computed for these outputs and, thus, they can not be expected to contain meaningful information.) "}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2025-11-28T13:27:08.973542Z", "start_time": "2025-11-28T13:27:08.969024Z"}}, "outputs": [], "source": "class SentimentPredictionModel(nn.Module):\n    def __init__(self, vocab_size: int, dmodel: int, dk: int, dv: int, positional_encodings: torch.Tensor):\n        super().__init__()\n        self.positional_encodings = positional_encodings\n        self.embeddings = torch.nn.Embedding(vocab_size, dmodel)\n        ### YOUR SOLUTION HERE\n        ### END OF SOLUTION\n        \n        self.prediction_head1 = nn.Linear(dmodel, 16)\n        self.prediction_head2 = nn.Linear(16, 2)\n\n    def forward(self, x):\n        layer_scores = []\n        ### YOUR SOLUTION HERE\n        ### END OF SOLUTION\n\n        z_class = z[0]\n        pred = F.relu(self.prediction_head1(z_class))\n        class_logits = self.prediction_head2(pred)\n        return class_logits, layer_scores"}, {"cell_type": "markdown", "metadata": {}, "source": "##### Sentiment Classification\n\nIn the final step we can now train our sentiment classification model on the movie review dataset. Therefore we split the entire dataset into a training and a validation dataset and instantiate DataLoader instances for both: ```train_loader``` and ```val_loader```."}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2025-11-28T13:27:11.026332Z", "start_time": "2025-11-28T13:27:08.974549Z"}}, "outputs": [], "source": "model = SentimentPredictionModel(len(encoding_dict), dmodel, dk, dv, encodings)\nlr = 1e-5\noptimizer = optim.Adam(model.parameters(), lr=lr)\ncriterion = nn.CrossEntropyLoss()\ndevice = 'cpu'\n\ntrain_idx = int(len(encoded_texts) * 0.5)\nencoded_texts_train = torch.tensor(encoded_texts[:train_idx])\nlabels_train = labels[:train_idx]\nencoded_texts_val = torch.tensor(encoded_texts[train_idx:])\nlabels_val = labels[train_idx:]"}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2025-11-28T13:27:11.031696Z", "start_time": "2025-11-28T13:27:11.027346Z"}}, "outputs": [], "source": "train_dataset = torch.utils.data.TensorDataset(encoded_texts_train, labels_train)\nval_dataset = torch.utils.data.TensorDataset(encoded_texts_val, labels_val)\n\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=1)\nval_loader = torch.utils.data.DataLoader(val_dataset)"}, {"cell_type": "markdown", "metadata": {"collapsed": false}, "source": "Inside the training loop, please implement the core training functionalities:\n * extract the movie review texts and the corresponding sentiment labels\n * pass the batch of review texts through the model to obtain the prediction ```out``` and the attention scores ```scores```\n * use the prediction and the groundtruth label to compute the ```loss``` value\n * execute one step of gradient descent (remember to call `zero_grad()` before executing the loss' `backward()` pass and computing the weight updates via `step()`)\n * add the loss computed after the forward pass of the current batch to ```episode_loss_train```."}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2025-11-28T13:27:39.142448Z", "start_time": "2025-11-28T13:27:11.032702Z"}}, "outputs": [], "source": "n_episodes = 100\nepisode_losses_train = []\nepisode_losses_val = []\n\nfor episode in range(n_episodes):\n    episode_loss_train = 0\n    episode_loss_val = 0\n    model.train()\n    for batch_idx, sample in enumerate(train_loader):\n        sample_x, sample_y = sample\n        ### YOUR SOLUTION HERE\n        ### END OF SOLUTION\n        \n    model.eval()\n    for batch_idx, sample in enumerate(val_loader):\n        sample_x, sample_y = sample\n        out, scores = model(sample_x.squeeze())\n        loss = criterion(out, sample_y[0])\n        episode_loss_val += loss.item()\n    episode_losses_train.append(episode_loss_train / len(encoded_texts_train))\n    episode_losses_val.append(episode_loss_val / len(encoded_texts_val))"}, {"cell_type": "markdown", "metadata": {"collapsed": false}, "source": "##### Loss Visualization\n\nLooking at the training and validation loss, what behaviour can you observe?"}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2025-11-28T13:27:39.213307Z", "start_time": "2025-11-28T13:27:39.143731Z"}}, "outputs": [], "source": "fig, axes = plt.subplots(1, 1)\naxes.plot(episode_losses_train)\naxes.plot(episode_losses_val)"}, {"cell_type": "markdown", "metadata": {"collapsed": false}, "source": "##### Score Visualization\n\nThe following code cells produce a visualization of the scores computed in the self-attention mechanism for a single training sample.\n\nIf you focus on the first row in the matrix-shaped color based visualization (the row for the `<cls>`-token), you can identify the sequence's tokens which the classifier attends to most.\n\nVisualize the attention scores for different data samples, investigate how they vary and which words seem to be important for the model's classification decision."}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2025-11-28T13:27:39.221481Z", "start_time": "2025-11-28T13:27:39.214313Z"}}, "outputs": [], "source": "encoded_text = list(encoded_texts_train[13].tolist())\nprint(encoded_text)\ndecoded_text = decode_sequence(encoded_text, decoding_dict)\nprint(decoded_text)\nout, scores = model(torch.tensor(encoded_text))"}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2025-11-28T13:27:39.335600Z", "start_time": "2025-11-28T13:27:39.222486Z"}}, "outputs": [], "source": "fig, axes = plt.subplots(1, 1)\nim = axes.imshow(scores[0].detach())\nplt.colorbar(im)"}]}