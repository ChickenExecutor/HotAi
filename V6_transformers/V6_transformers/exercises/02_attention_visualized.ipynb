{"metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}}, "nbformat": 4, "nbformat_minor": 2, "cells": [{"cell_type": "markdown", "metadata": {"collapsed": false}, "source": "# Attention Visualization with BERTViz\n\nThis notebook demonstrates how to visualize attention mechanisms in transformer models using the BERTViz library. We will cover both encoder-only models (like BERT) and encoder-decoder models (like translation models).\n\nIn both cases, a pretrained transformer model from the [Hugging Face Transformers library](https://huggingface.co/docs/transformers/index) is used, and attention weights are extracted during a forward pass. The BERTViz library provides interactive visualizations to explore these attention weights."}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2025-12-01T10:14:36.556383Z", "start_time": "2025-12-01T10:14:06.104338Z"}, "collapsed": false}, "outputs": [], "source": "!pip install torchvision==0.21.0\n!pip install torch==2.6.0\n!pip install transformers==4.57.3\n!pip install bertviz==1.4.1"}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2025-12-01T10:15:41.814037Z", "start_time": "2025-12-01T10:14:36.559916Z"}}, "outputs": [], "source": "from transformers import AutoTokenizer, AutoModel, utils\nfrom bertviz import model_view\nfrom bertviz import head_view\nimport matplotlib.pyplot as plt\nutils.logging.set_verbosity_error()  # Remove line to see warnings"}, {"cell_type": "markdown", "metadata": {}, "source": "#### Encoder Model Visualization"}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2025-12-01T10:15:41.818598Z", "start_time": "2025-12-01T10:15:41.817599Z"}}, "outputs": [], "source": "utils.logging.set_verbosity_error()  # Suppress standard warnings\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\nmodel = AutoModel.from_pretrained(\"bert-base-uncased\", output_attentions=True)"}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2025-12-01T10:15:41.820608Z", "start_time": "2025-12-01T10:15:41.819603Z"}}, "outputs": [], "source": "inputs = tokenizer.encode(\"The kid likes to go to school because it likes to learn new things.\", return_tensors='pt')\noutputs = model(inputs)\nattention = outputs[-1]  # Output includes attention weights when output_attentions=True\ntokens = tokenizer.convert_ids_to_tokens(inputs[0])"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "head_view(attention, tokens)"}, {"cell_type": "markdown", "metadata": {}, "source": "#### Encoder-Decoder Model Visualization"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-de\")\nmodel = AutoModel.from_pretrained(\"Helsinki-NLP/opus-mt-en-de\", output_attentions=True)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "encoder_input_ids = tokenizer(\"She sees the small elephant.\", return_tensors=\"pt\", add_special_tokens=True).input_ids\nwith tokenizer.as_target_tokenizer():\n    decoder_input_ids = tokenizer(\"Sie sieht den kleinen Elefanten.\", return_tensors=\"pt\", add_special_tokens=True).input_ids\n\noutputs = model(input_ids=encoder_input_ids, decoder_input_ids=decoder_input_ids)\n\nencoder_text = tokenizer.convert_ids_to_tokens(encoder_input_ids[0])\ndecoder_text = tokenizer.convert_ids_to_tokens(decoder_input_ids[0])\n\nmodel_view(\n    encoder_attention=outputs.encoder_attentions,\n    decoder_attention=outputs.decoder_attentions,\n    cross_attention=outputs.cross_attentions,\n    encoder_tokens= encoder_text,\n    decoder_tokens = decoder_text\n)"}]}