{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# LSTM Language Model - Inference\n",
    "\n",
    "In this tutorial, we will focus on a LSTM language model trained on the open source dataset wikitext-103.\n",
    "The dataset consists of verified wikipedia articles and contains more than 100 million tokens.\n",
    "\n",
    "As the complexity of dataset and model do not allow for a quick model training on commonly available hardware resources, we will consider only the set-up of data and models, and work with a pretrained model. \n",
    "The training was performed on a single GPU unit for 12 epochs.\n",
    " \n",
    "We will see that the resulting model incorporates some text generation abilities but certainly lacks any language proficiency.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T10:06:49.753731Z",
     "start_time": "2025-11-25T10:06:49.749730Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torchtext\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## The Dataset\n",
    "\n",
    "The attached file `data_processing.py` contains functions for loading and processing the data of the **wikitext-103** dataset, which consists of verified Wikipedia articles.\n",
    "\n",
    "More information on the dataset [is available here](https://www.kaggle.com/datasets/vadimkurochkin/wikitext-103).\n",
    "\n",
    "The data itself can be found in this directory's subfolder `data/wikitext-103`.\n",
    "It consists of three data files: `wiki.train.tokens`, `wiki.valid.tokens` and `wiki.test.tokens`, containing train, validation and test data, respectively.\n",
    "\n",
    "As in the previous exercise, the basic english tokenizer as provided by `torchtext` was used for text tokenization.\n",
    "\n",
    "We load the vocabulary built from the dataset's training split as instance of torchtext's class `Vocab`.\n",
    "\n",
    "As the training split is very large, we do not load it at this point. \n",
    "Instead, we use the function `load_dataset_from_file` to read the test data file, encode it according to tokenizer and vocabulary and load it to a PyTorch-compatible data class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T10:06:49.859187Z",
     "start_time": "2025-11-25T10:06:49.779661Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from data_processing import load_vocab, load_dataset_from_file, data_dir_wikitext_103 as data_dir\n",
    "tokenizer = torchtext.data.utils.get_tokenizer('basic_english')\n",
    "vocab = load_vocab(os.path.join(data_dir, \"vocab.pkl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We can query the size of the vocabulary as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T10:06:49.864442Z",
     "start_time": "2025-11-25T10:06:49.860192Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "A token (word) can be encoded as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T10:06:49.873943Z",
     "start_time": "2025-11-25T10:06:49.865448Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab[\"something\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "and decoded as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T10:06:49.892459Z",
     "start_time": "2025-11-25T10:06:49.875949Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab.get_itos()[1001]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The data is organized in sentences where each sentence is started with a dedicated start (`<sos>`) and ended with a dedicated end (`<eos>`) token.\n",
    "Each sentence is padded to identical length, so that arbitrary sequences can be batched in a single tensor. \n",
    "Therefore, a pad token (`<pad>`) is used. \n",
    "The token indicates the end of the sequence has been reached, and it is explicitly ignored on model training and evaluation (when computing losses, gradients and metrics). \n",
    "\n",
    "Each data sample contains the complete encoded sequence as x-value.\n",
    "The corresponding y-value (target) is the same sequence shifted to the left.\n",
    "This is as in every step, one token of the input is fed into the recurrent LSTM model, and the next sequence token is the model's expected output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T10:07:48.335744Z",
     "start_time": "2025-11-25T10:07:48.230121Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The following code only works if the wikitext-103 dataset is available in the specified data_dir.\n",
    "# Otherwise, this cell can be ignored. The rest of the notebook works without loading the dataset.\n",
    "dataset_test = load_dataset_from_file(os.path.join(data_dir, \"wiki.test.tokens\"), tokenizer, vocab, max_seq_length=500, pad_token_index=vocab[\"<pad>\"])\n",
    "sample_x, sample_y = dataset_test[0]\n",
    "assert torch.all(sample_x[1:] == sample_y[0:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Restoring the pretrained model\n",
    "\n",
    "To be able to load the pretrained model weights, the identical model architecture needs to be constructed first. \n",
    "\n",
    "The following class, `LM_LSTM_Model` contains the model architecture of our pretrained model.\n",
    "\n",
    "Inspect the architecture and notice the use of the LSTM-class provided by `PyTorch`.\n",
    "Especially note that the `forward()`-method takes an additional argument: `hidden`. \n",
    "This argument contains the networks hidden states (both hidden and context layers) after processing of the previous sequence element.\n",
    "\n",
    "Note that the hidden state has to be set to zero before processing a new sequence.\n",
    "To initialize new hidden states, the method `init_hidden` is provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T10:06:50.007776Z",
     "start_time": "2025-11-25T10:06:50.003598Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LM_LSTM_Model(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, dropout_rate):\n",
    "        super(LM_LSTM_Model, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "        embedding = torch.nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.embedding = embedding\n",
    "        self.lstm = torch.nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers,\n",
    "                                  dropout=dropout_rate, batch_first=True)\n",
    "        self.dropout = torch.nn.Dropout(dropout_rate)\n",
    "        self.fc = torch.nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        embedding = self.dropout(self.embedding(x))\n",
    "        output, hidden = self.lstm(embedding, hidden)\n",
    "        output = self.dropout(output)\n",
    "        prediction = self.fc(output)\n",
    "        return prediction, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size, device=\"cpu\"):\n",
    "        hidden = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device)\n",
    "        cell = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device)\n",
    "        return hidden, cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We initialize the model (using the same parameters as in training) and set it to evaluation mode.\n",
    "(In evaluation mode, training functionalities are disabled, e.g. gradient computation is not activated and dropout is not applied.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T10:06:51.135199Z",
     "start_time": "2025-11-25T10:06:50.008782Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embedding_dim = 512\n",
    "hidden_dim = 1024\n",
    "num_layers = 3\n",
    "\n",
    "model = LM_LSTM_Model(len(vocab), embedding_dim, hidden_dim, num_layers, 0)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Next, we load the model weights thereby restoring the model trained on our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T10:06:51.774474Z",
     "start_time": "2025-11-25T10:06:51.136205Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "state_dict = torch.load(f\"./pretrained_model/lstm_model_weights.pkl\")\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The plots of the model's training and evaluation loss over the trained epochs show that the LSTM model at hand does not suffer from excessive overfitting as seen for the neural ngram model previously.\n",
    "\n",
    "![image](images/lstm_model_loss_train.png) ![image](images/lstm_model_loss_eval.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We want to use the pretrained model to create some texts.\n",
    "\n",
    "Therefore, we implement the function `generate_text(model, vocab, start_tokens, max_iterations)` which takes the model and its vocabulary as first two input parameters.\n",
    "Further, the start of a sequence which the model will be continuing is input as list of strings in `start_tokens`.\n",
    "The parameter `max_iterations` contains the maximum number of additional tokens to be generated by the model.\n",
    "\n",
    "Complete the function according to the instructions in the line comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T10:06:51.780895Z",
     "start_time": "2025-11-25T10:06:51.775607Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_text(model, vocab, start_tokens, max_iterations):\n",
    "    model.eval()\n",
    "    \n",
    "    # encode the start sequence using the vocabulary\n",
    "    start_tokens_encoded = vocab(start_tokens)\n",
    "    \n",
    "    # initialize variable to hold the complete generated sequence as encoded tokens (numerical) (needs to be concatenated to as generation proceeds) \n",
    "    complete_sequence = torch.tensor(start_tokens_encoded)\n",
    "\n",
    "    # Initialize the LSTM's hidden units for the new sequence\n",
    "    hidden = model.init_hidden(1)\n",
    "    # start solution\n",
    "    # Create a tensor containing the first tokens (start_tokens) and run them through the network to obtain the corresponding hidden state\n",
    "    start_tokens_tensor = torch.tensor([start_tokens_encoded])\n",
    "    _, hidden = model(start_tokens_tensor, hidden)\n",
    "    # end solution\n",
    "    \n",
    "    # Iteratively create next tokens and append them to the complete sequence.\n",
    "    # Stop if \"<eos>\" is predicted or when the maximum number of iterations is reached.\n",
    "    for i in range(max_iterations):\n",
    "        # start solution\n",
    "        # 1) extract next model input, i.e. last token of complete sequence. (in each iteration, only the last token, which was not yet passed through the network, needs to be fed to the model.) \n",
    "        model_input = complete_sequence[-1:].unsqueeze(0)\n",
    "        # 2) run model on next token (update hidden state variable)\n",
    "        predictions, hidden = model(model_input, hidden)\n",
    "        # 3) mask the prediction for the default-token \"<unk>\" as we do not want our model to predict this token\n",
    "        predictions[0, -1, vocab[\"<unk>\"]] = 0\n",
    "        # 4) find the token with the highest predicted score\n",
    "        next_word_code = predictions[0, -1, :].argmax()\n",
    "        # end solution\n",
    "        \n",
    "        # add next predicted token to tensor containing the complete sequence\n",
    "        complete_sequence = torch.concat([complete_sequence, next_word_code.unsqueeze(0)], -1)\n",
    "\n",
    "        # stop, if \"<eos>\" is predicted\n",
    "        if next_word_code in vocab([\"<eos>\"]):\n",
    "            break\n",
    "\n",
    "    # decode complete sequence to human-readable tokens\n",
    "    result_tokens = [vocab.get_itos()[i] for i in complete_sequence]\n",
    "    result_sequence = \" \".join(result_tokens)\n",
    "    return result_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We can use the function above to test the pretrained language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T10:06:54.435354Z",
     "start_time": "2025-11-25T10:06:51.781904Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sequence_start = tokenizer(\"<sos> the quick brown fox\")\n",
    "generate_text(model, vocab, sequence_start, 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Model Embeddings\n",
    "\n",
    "As seen in the previous exercise, we can visualize the embeddings inherent in the model (learned simultaneously with the language model as a whole).\n",
    "\n",
    "Below, we perform principal component analysis (PCA) to reduce the dimensionality of the embeddings to two dimensions for easier plotting. \n",
    "We create a scatter plot to visualize the spatial relationships between selected words.\n",
    "\n",
    "Considering the arrangement of word embeddings in space, what can you say about the learned embeddings? How do they compare to the sophisticated GloVe embeddings inspected in the previous exercise session?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T10:06:54.439638Z",
     "start_time": "2025-11-25T10:06:54.436359Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_token_embedding(token: str, vocab, model):\n",
    "    encoded_token = vocab[token]\n",
    "    model_input = torch.tensor([encoded_token])\n",
    "    embedding = model.embedding(model_input)\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T10:06:54.457942Z",
     "start_time": "2025-11-25T10:06:54.440643Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_word_embeddings(words, embeddings):\n",
    "    word_vectors = [embeddings[word] for word in words if word in embeddings]\n",
    "    pca = PCA(n_components=2)\n",
    "    word_vectors_2d = pca.fit_transform(word_vectors)\n",
    "    \n",
    "    plt.figure(figsize=(10, 10))\n",
    "    for i, word in enumerate(words):\n",
    "        if word in embeddings:\n",
    "            plt.scatter(word_vectors_2d[i, 0], word_vectors_2d[i, 1])\n",
    "            plt.text(word_vectors_2d[i, 0] + 0.01, word_vectors_2d[i, 1] + 0.01, word, fontsize=9)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T10:06:54.465838Z",
     "start_time": "2025-11-25T10:06:54.458947Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "words = [\"king\", \"queen\", \"mother\", \"father\", \"soda\", \"coke\", \"france\", \"spain\"]\n",
    "embeddings = {token: get_token_embedding(token, vocab, model).squeeze(0).detach().numpy() for token in words}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T10:06:54.549121Z",
     "start_time": "2025-11-25T10:06:54.466851Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_word_embeddings(words, embeddings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
