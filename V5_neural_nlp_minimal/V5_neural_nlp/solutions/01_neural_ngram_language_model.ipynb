{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# NGram Neural Language Model - Solutions\n",
    "\n",
    "In this exercise, we will set up a neural ngram language model and prepare corresponding training and test data based on a short sample text.\n",
    "We will train the model on our very small corpus and perform inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Package Setup\n",
    "\n",
    "If not already done, please install the following packages to your python interpreter. (Remove the leading comment symbols `#` to execute the installation commands.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T09:27:09.884521Z",
     "start_time": "2025-11-25T09:27:09.880280Z"
    }
   },
   "outputs": [],
   "source": [
    "#!pip install numpy==1.26.4\n",
    "#!pip install torch==2.2.2\n",
    "#!pip install torchtext==0.17.2\n",
    "#!pip install matplotlib\n",
    "#!pip install tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Package Imports\n",
    "\n",
    "Execute the follwoing cell to load all required packages to your running interpreter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T09:27:15.035524Z",
     "start_time": "2025-11-25T09:27:09.885533Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Dict, List\n",
    "\n",
    "import torch\n",
    "import torchtext\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Preparation of training data\n",
    "\n",
    "Before we can start to implement and train a language model, the training data needs to be considered and preprocessed adequately.\n",
    "\n",
    "In this tutorial, we will be working with a small sample text taken from a newspaper article from October 2024.\n",
    "The sample text can be found in this notebook's directory's sub folder `data`.\n",
    "We suggest to manually inspect the text file to get an idea of its contents.\n",
    "\n",
    "The following code lines define the relative paths to the source file. \n",
    "Depending on your environment and IDE, you might need to adjust the paths according to your current working directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T14:23:22.476739Z",
     "start_time": "2025-11-25T14:23:22.455221Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from data_processing import data_dir\n",
    "train_text_file_path = os.path.join(data_dir, \"sample_text.txt\")\n",
    "\n",
    "assert os.path.isfile(train_text_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "If the previous cell produced an error in the final assert statement, please check your current working directory with the subsequent code cell. \n",
    "Adjust the file paths above accordingly (so they contain the data file paths relative to your current directory). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T09:27:15.049641Z",
     "start_time": "2025-11-25T09:27:15.042337Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Next, we will define a function to read the text contents of our data file and read the file contents into a single string variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T09:27:15.064646Z",
     "start_time": "2025-11-25T09:27:15.050647Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_text_file(text_file_path: str) -> List[str]:\n",
    "    with open(text_file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        raw_text = file.read()\n",
    "    return raw_text\n",
    "\n",
    "raw_text = read_text_file(train_text_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "To ensure the text was read properly and get an idea what the text looks like, let's print the first 500 characters of the read text: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T09:27:15.070247Z",
     "start_time": "2025-11-25T09:27:15.065854Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "raw_text[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "In the next step, the text needs to be tokenized, i.e. split into small units like words (or subwords).\n",
    "\n",
    "The most naive approach would be to split the text at every whitespace and use each word as one token.\n",
    "\n",
    "The following code performs such a naive tokenization and assembles a sorted list of all extracted tokens.\n",
    "Looking at a small excerpt of this list already reveals some problems of this naive approach.\n",
    "What are they?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T09:27:15.077091Z",
     "start_time": "2025-11-25T09:27:15.071253Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokens_naive = raw_text.split()\n",
    "tokens_naive_set = set().union(tokens_naive)\n",
    "tokens_naive_sorted = sorted(tokens_naive_set)\n",
    "sorted(tokens_naive_set)[36:40]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The package torchtext provides out-of-the-box tokenizers for different languages and cases.\n",
    "We will use the tokenizer for basic english language and employ it to tokenize our sentences.\n",
    "This tokenizer already handles important issues like letter casing, punctuation characters, or quotation marks.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T09:27:15.083481Z",
     "start_time": "2025-11-25T09:27:15.078097Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenizer = torchtext.data.utils.get_tokenizer('basic_english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The tokenizer can be applied to a text, thereby converting it to a list of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T09:27:15.089793Z",
     "start_time": "2025-11-25T09:27:15.084494Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenized_text = tokenizer(raw_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "If we compare the numbers of unique tokens of the naive tokenization via the torchtext-provided tokenization, we will see that the number of unique tokens has indeed decreased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T09:27:15.096948Z",
     "start_time": "2025-11-25T09:27:15.090799Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(len(tokens_naive_sorted))\n",
    "print(len(sorted(set().union(tokenized_text))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "In this tutorial/demonstration case, we will split the data into a training and a test set in a very simple way:\n",
    "The first 80% of the token sequence will be used for training, and the remaining 20% for test purposes.\n",
    "\n",
    "(In real-world scenarios, a more sophisticated split, depending on the data structure, should be applied.\n",
    "In most cases, a threefold split into train-validation-test is advisable.)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T09:27:15.102947Z",
     "start_time": "2025-11-25T09:27:15.097953Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_amount = int(0.8 * len(tokenized_text))\n",
    "tokenized_text_train = tokenized_text[:train_amount]\n",
    "tokenized_text_test = tokenized_text[train_amount:]\n",
    "\n",
    "print(\"Token amount: Train {} / Test {}\".format(len(tokenized_text_train), len(tokenized_text_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Vocabulary Definition\n",
    "\n",
    "In the following, we will use the training data to define the vocabulary and the procedure for transforming token to numeric representations. \n",
    "\n",
    "First, we will manually build the vocabulary based on the sequence of tokens.\n",
    "\n",
    "To do so, we first define a function which extracts all unique tokens and counts their number of appearances.\n",
    "Thus, the function `get_vocabulary_and_frequencies(token_sequence)` receives a sequence of tokens and returns a dictionary.\n",
    "Each of the dictionary's `str`-keys corresponds to a unique token, the corresponding value counts how often the token appears within the given sequence.\n",
    "\n",
    "Implement the function accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T09:27:15.109958Z",
     "start_time": "2025-11-25T09:27:15.103953Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_vocabulary_and_frequencies(token_sequence: List[str]) -> Dict[str, int]:\n",
    "    vocab = {}\n",
    "    # start solution\n",
    "    for item in token_sequence:\n",
    "        if item not in vocab:\n",
    "            vocab[item] = 0\n",
    "        vocab[item] += 1\n",
    "    # end solution\n",
    "    print(f\"Created vocabulary of {len(vocab)} unique tokens.\")\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Execute and test the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T09:27:15.116936Z",
     "start_time": "2025-11-25T09:27:15.110462Z"
    }
   },
   "outputs": [],
   "source": [
    "vocab = get_vocabulary_and_frequencies(tokenized_text_train)\n",
    "\n",
    "assert len(vocab) == 484\n",
    "assert vocab[\"\\'\"] == 17\n",
    "assert vocab[\"biden\"] == 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Next, we will use the created token-frequency-dictionary to create dictionaries for encoding and decoding tokens to numerics and vice versa. \n",
    "\n",
    "To do so, we implement the function `get_encoding_decoding_dicts(vocabulary)`.\n",
    "It receives the token-frequency-dictionary as input and produces two dictionaries:\n",
    "\n",
    "* One dictionary for encoding: The keys are the unique tokens in the vocabulary, the values are unique integers assigned to each token.\n",
    "* One dictionary for decoding: It reverses the key-item-relations of the encoding dictionary.\n",
    "\n",
    "When building the encoding dictionary for the `n` tokens within the vocabulary, choose the integers from `0` to `n-1` as values. \n",
    "\n",
    "Additionally, the token `\"<unk>\"` will be added to the vocabulary. \n",
    "It will be used as default token in case of unknown tokens (e.g. words not contained in the training data). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T09:27:15.123589Z",
     "start_time": "2025-11-25T09:27:15.117941Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_encoding_decoding_dicts(vocabulary: Dict[str, int]):\n",
    "    vocab_encoding = {}\n",
    "    vocab_decoding = {}\n",
    "    # start solution\n",
    "    for i, key in enumerate(vocabulary.keys()):\n",
    "        vocab_encoding[key] = i\n",
    "        vocab_decoding[i] = key\n",
    "    # end solution\n",
    "    unk_index = len(vocab_encoding)\n",
    "    vocab_encoding[\"<unk>\"] = unk_index\n",
    "    vocab_decoding[unk_index] = \"<unk>\"\n",
    "    return vocab_encoding, vocab_decoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Now, we can call the function to build the encoding and decoding dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T09:27:15.129031Z",
     "start_time": "2025-11-25T09:27:15.124594Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "encoding_vocab, decoding_vocab = get_encoding_decoding_dicts(vocab)\n",
    "\n",
    "assert len(encoding_vocab) == len(decoding_vocab)\n",
    "assert \"biden\" == decoding_vocab[encoding_vocab[\"biden\"]]\n",
    "assert \"\\'\" == decoding_vocab[encoding_vocab[\"\\'\"]]\n",
    "assert 123 == encoding_vocab[decoding_vocab[123]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The encoding dictionary can be used to convert the tokenized training text to numerical values, which can be used as input to our language model.\n",
    "\n",
    "Implement the function `encode_token_sequence(tokenized_sequence: List[str], encoding_dict:Dict[str, int], default_token = \"<unk>\")` which encodes a sequence of tokens based on the given dictionary.\n",
    "Whenever the sequence contains a token which is not contained in the encoding dictionary, use the default token instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T09:27:15.136217Z",
     "start_time": "2025-11-25T09:27:15.130037Z"
    }
   },
   "outputs": [],
   "source": [
    "def encode_token_sequence(tokenized_sequence: List[str], encoding_dict:Dict[str, int], default_token = \"<unk>\"):\n",
    "    # query default token encoding once\n",
    "    default_token_encoded = encoding_dict[default_token]\n",
    "    \n",
    "    encoded_sequence = []\n",
    "    # iterate through token sequence and encode tokens one by one\n",
    "    # start solution\n",
    "    for token in tokenized_sequence:\n",
    "        if token in encoding_dict:\n",
    "            encoded_token = encoding_dict[token]\n",
    "        else:\n",
    "            encoded_token = default_token_encoded\n",
    "        encoded_sequence.append(encoded_token)\n",
    "    # end solution\n",
    "    return encoded_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We apply the just implemented function `encode_token_sequence` and the encoding dictionary to encode our training and test token sequences to numerical arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T09:27:15.142551Z",
     "start_time": "2025-11-25T09:27:15.137221Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "encoded_data_train = encode_token_sequence(tokenized_text_train, encoding_vocab, default_token = \"<unk>\")\n",
    "encoded_data_test = encode_token_sequence(tokenized_text_test, encoding_vocab, default_token = \"<unk>\")\n",
    "\n",
    "assert len(encoded_data_train) == len(tokenized_text_train)\n",
    "assert len(encoded_data_test) == len(tokenized_text_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Next, we build our own PyTorch-compatible `Dataset`-class `NGramDataset` for n-gram text data.\n",
    "\n",
    "In general, a `Dataset` class has access to a complete data set and makes its samples accessible to other PyTorch classes and functions. \n",
    "\n",
    "To be able to use custom organized data with PyTorch and its training functionalities, one can implement a subclass of `torch.utils.data.Dataset`.\n",
    "Thereby, it is mandatory to implement class initializer `__init__` and the methods `__getitem__` and `__len__`.\n",
    "\n",
    "In our case, the class initializer takes the tensor containing the complete encoded corpus and the n-gram size.\n",
    "Both are stored within the class as member variables and are then easily accessible within other class methods.\n",
    "\n",
    "The method `__getitem__` receives an integer index as input and returns the corresponding data sample, i.e. the corresponding `x`- and `y`-values.\n",
    "In case of n-gram data, the `x` (input) value are `n-1` coherent entries in the data, and the corresponding `y` (target) value is the next subsequent entry.\n",
    "Moreover, for `n=4`, the call of `__getitem__()` with `index=0` would return a tuple of a list of the first three encoded tokens in the data as first value, and the third encoded token as second value.\n",
    "\n",
    "The method `__len__` has no arguments and returns the number of samples within the dataset.\n",
    "\n",
    "Implement the two methods below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T09:27:15.149545Z",
     "start_time": "2025-11-25T09:27:15.143557Z"
    }
   },
   "outputs": [],
   "source": [
    "class NGramDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, corpus, ngram_size):\n",
    "        super(NGramDataset).__init__()\n",
    "        self.ngram_size = ngram_size\n",
    "        self.complete_corpus = torch.tensor(corpus)\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # start solution\n",
    "        return self.complete_corpus[index:index+self.ngram_size-1], self.complete_corpus[index+self.ngram_size-1]\n",
    "        # end solution\n",
    "    \n",
    "\n",
    "    def __len__(self):\n",
    "        # start solution\n",
    "        return len(self.complete_corpus) - (self.ngram_size - 1)\n",
    "        # end solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Now that the dataset class is defined, we pour our training and test data into it.\n",
    "\n",
    "Further, we choose `n=5`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T09:27:15.157549Z",
     "start_time": "2025-11-25T09:27:15.150550Z"
    }
   },
   "outputs": [],
   "source": [
    "ngram_length = 5\n",
    "\n",
    "dataset_train = NGramDataset(encoded_data_train, ngram_length)\n",
    "dataset_test = NGramDataset(encoded_data_test, ngram_length)\n",
    "\n",
    "assert len(dataset_train) == 1200\n",
    "assert len(dataset_test) == 297"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Next, we use PyTorch's `DataLoader` to provide batches of data for the training and evaluation process.\n",
    "Also, shuffling of training data with each epoch is already implemented in this class.\n",
    "\n",
    "When training and evaluation our model, we will be able to iterate over these dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T09:27:15.162770Z",
     "start_time": "2025-11-25T09:27:15.157549Z"
    }
   },
   "outputs": [],
   "source": [
    "dl_train = torch.utils.data.DataLoader(dataset_train, shuffle=True, batch_size=32)\n",
    "dl_test = torch.utils.data.DataLoader(dataset_test, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Model Definition and Implementation\n",
    "\n",
    "Now, we define our neural n-gram model as a subclass of `torch.nn.Module` (compare to previous exercises).\n",
    "\n",
    "Our neural n-gram model consists of an embedding-layer, which projects the encoded input tokens in their numeric represantation to a multidimensional embedding space.\n",
    "Next, a fully-connected layer with ReLU activation and another fully-connected layer follow.\n",
    "The latter fully connected layer's output dimension corresponds to the number of unique tokens in the vocabulary with each unit giving a score for the corresponding token in the vocabulary.\n",
    "\n",
    "In the following, the class initializer, which initialize all the required neural network components is already implemented.\n",
    "Implement the class' `forward()`-method which computes the model's forward pass.\n",
    "The input to the `forward()`-function can be assumed to be of shape `(batch_size, n-1)`.\n",
    "\n",
    "Note that the embeddings layer produces a 3-dimensional tensor of shape `(batch_size, n-1, embedding_dim)`.\n",
    "This tensor needs to be flattened to 2 dimensions (`(batch_size, (n-1) * embedding_dim)`) before it can be input into the first linear layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T09:27:15.169220Z",
     "start_time": "2025-11-25T09:27:15.163780Z"
    }
   },
   "outputs": [],
   "source": [
    "class NeuralNgram(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, ngram_size: int, vocab_size: int, embedding_dim: int = 64, hidden_dim: int = 128):\n",
    "        super(NeuralNgram, self).__init__()\n",
    "        self.ngram_size = ngram_size\n",
    "        self.embeddings = torch.nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear_1 = torch.nn.Linear((ngram_size - 1) * embedding_dim, hidden_dim)\n",
    "        self.activation = torch.nn.ReLU()\n",
    "        self.linear_2 = torch.nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # start solution\n",
    "        x_emb = self.embeddings(x).view((x.shape[0], -1))\n",
    "        x_lin_1 = self.linear_1(x_emb)\n",
    "        x_out_1 = self.activation(x_lin_1)\n",
    "        output = self.linear_2(x_out_1)\n",
    "        return output\n",
    "        # end solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "A simple test for your implementation follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T09:27:15.188135Z",
     "start_time": "2025-11-25T09:27:15.170236Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = NeuralNgram(ngram_length, len(encoding_vocab))\n",
    "\n",
    "test_tensor = torch.zeros((32, ngram_length - 1)).long()\n",
    "model_out = model(test_tensor)\n",
    "\n",
    "assert model_out.shape[0] == 32\n",
    "assert model_out.shape[1] == len(encoding_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We instantiate our model as instance of the class `NeuralNgram`.\n",
    "When initializing the class, `n=5` is set according to the choice above and the number of unique tokens in our vocabulary is passed as input parameter.\n",
    "\n",
    "Further, we initialize the Cross Entropy Loss and a stochastic gradient descent optimizer for the model's parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T09:27:16.042247Z",
     "start_time": "2025-11-25T09:27:15.189149Z"
    }
   },
   "outputs": [],
   "source": [
    "model = NeuralNgram(ngram_length, len(encoding_vocab))\n",
    "\n",
    "loss = torch.nn.CrossEntropyLoss(reduction='sum')\n",
    "optimizer = torch.optim.SGD(lr=0.01, params=model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T09:27:20.331188Z",
     "start_time": "2025-11-25T09:27:16.042247Z"
    }
   },
   "outputs": [],
   "source": [
    "n_epochs = 30\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "for i in range(n_epochs):\n",
    "    # Perform training epoch\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for data_x, data_y in dl_train:\n",
    "        # start solution\n",
    "        # 1) run model on data\n",
    "        predictions = model(data_x)\n",
    "        # 2) compute loss (note: target values need to be converted to long before being passed to the cross-entropy-loss-function)\n",
    "        batch_loss = loss(predictions, data_y.long())\n",
    "        # end solution\n",
    "        epoch_loss += batch_loss.cpu().detach().numpy()\n",
    "        optimizer.zero_grad()\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    epoch_loss = epoch_loss / len(dataset_train)\n",
    "    train_losses.append(epoch_loss)\n",
    "\n",
    "    # Evaluate on test data\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    for test_x, test_y in dl_test:\n",
    "        predictions = model(test_x)\n",
    "        batch_loss = loss(predictions, test_y.long())\n",
    "        test_loss += batch_loss.cpu().detach().numpy()\n",
    "\n",
    "    test_loss = test_loss / len(dataset_test)\n",
    "    test_losses.append(test_loss)\n",
    "\n",
    "    if i%5 == 0:\n",
    "        print(\"Epoch {}\".format(i+1))\n",
    "        print(\"Train loss: {}\".format(epoch_loss))\n",
    "        print(\"Test Loss: {}\".format(test_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Looking at the evolution of training and test loss, it becomes obvious that overfitting is an issue in our training pipeline.\n",
    "\n",
    "What do you think is the main reason for overfitting in this case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T09:27:20.463634Z",
     "start_time": "2025-11-25T09:27:20.332203Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(train_losses, label=\"Train Loss\")\n",
    "plt.plot(test_losses, label=\"Test Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Next, we want to compute the model's perplexity on training and test data.\n",
    "\n",
    "Therefore, we compute the perplexity using the cross entropy loss: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T09:27:20.467863Z",
     "start_time": "2025-11-25T09:27:20.464640Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_perplexity(model, data_loader):\n",
    "    loss = torch.nn.CrossEntropyLoss(reduction=\"sum\")\n",
    "    loss_sum = 0\n",
    "    for data_x, data_y in data_loader:\n",
    "        scores = model(data_x)\n",
    "        loss_sum += loss(scores, data_y.long()) \n",
    "\n",
    "    perplexity = torch.exp(loss_sum / len(data_loader.dataset))\n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T09:27:20.520872Z",
     "start_time": "2025-11-25T09:27:20.468870Z"
    }
   },
   "outputs": [],
   "source": [
    "ppl_train = compute_perplexity(model, dl_train)\n",
    "print(ppl_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T09:27:20.536620Z",
     "start_time": "2025-11-25T09:27:20.522193Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ppl_test = compute_perplexity(model, dl_test)\n",
    "print(ppl_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Both loss values output during training and perplexity values again indicate that our model tremendously overfitted the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Nevertheless, we want to implement inference on our neural n-gram model and have the model generate a few lines of text.\n",
    "\n",
    "The function `inference(ngram_words, encoding_vocab, decoding_vocab, model)` takes as arguments:\n",
    "* A list of n-1 tokens which correspond to the beginning of an arbitrary n-gram.\n",
    "* Encoding and decoding vocabulary to convert tokens to numerical representations and vice versa\n",
    "* The trained n-gram model\n",
    "\n",
    "It queries the model's predictions for the n-gram and extracts the token with the highest score, which is returned by the function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T09:27:20.540985Z",
     "start_time": "2025-11-25T09:27:20.537629Z"
    }
   },
   "outputs": [],
   "source": [
    "def inference(ngram_words, encoding_vocab, decoding_vocab, model):\n",
    "    ngram_tokens = [encoding_vocab[token] for token in ngram_words]\n",
    "    token_tensor = torch.Tensor([ngram_tokens]).long()\n",
    "    # start solution\n",
    "    # Execute model and find token (word) with the highest predicted score\n",
    "    token_probs = model(token_tensor)\n",
    "    next_token = token_probs.argmax()\n",
    "    next_word = decoding_vocab[int(next_token)]\n",
    "    # end solution\n",
    "    return next_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Next, we define a starting n-gram (4 words from the dictionary) and let our model predict next words iteratively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T09:27:20.582381Z",
     "start_time": "2025-11-25T09:27:20.541992Z"
    }
   },
   "outputs": [],
   "source": [
    "start_ngram = [\"the\", \"presidential\", \"election\", \"was\"]\n",
    "current_ngram = start_ngram\n",
    "complete_sequence = \" \".join(start_ngram) + \" \"\n",
    "for i in range(100):\n",
    "    # start solution\n",
    "    # have the model make a word prediction\n",
    "    next_word = inference(current_ngram, encoding_vocab, decoding_vocab, model)\n",
    "    # update current_ngram for the next iteration\n",
    "    current_ngram = current_ngram[1:]\n",
    "    current_ngram.append(next_word)\n",
    "    # append the predicted word to the complete sequence (type str, human-readable)\n",
    "    complete_sequence += next_word + \" \"\n",
    "    # end solution\n",
    "print(complete_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The model predictions reflect the very small amount of training data on which the model has tremendously overfitted.\n",
    "That is, after few iterations, it starts repeating complete text passages from the training data.\n",
    "\n",
    "Still, compared to statistical n-gram models, it is able to process n-grams which were not previously seen in the training data (\"the presedential election was\" is not a sequence present in our sample text). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Summary of Results and Observations\n",
    "\n",
    "#### Model Performance:  \n",
    "* The training loss decreased steadily over epochs, indicating that the model successfully learned patterns from the training data.\n",
    "* However, the test loss and perplexity values suggest significant overfitting due to the small dataset size.\n",
    "\n",
    "#### Overfitting:  \n",
    "* The model memorized the training data, as evidenced by the repetitive text generation during inference.\n",
    "* This overfitting is likely caused by the limited training data and the absence of regularization techniques.\n",
    "\n",
    "#### Inference Results:  \n",
    "* The model was able to generate coherent sequences for n-grams not seen during training, demonstrating its ability to generalize to some extent.\n",
    "* However, the generated text quickly devolved into repetitive patterns, reflecting the overfitting issue.\n",
    "\n",
    "#### Key Takeaways:\n",
    "* A complete pipeline for implementing, training, and evaluating a simple neural n-gram model was demonstrated, showcasing the steps from data preprocessing to inference. \n",
    "* Increasing the dataset size or using a more diverse corpus would likely improve generalization.\n",
    "* Regularization techniques such as dropout, weight decay, or early stopping could help mitigate overfitting.\n",
    "* Experimenting with different model architectures or hyperparameters (e.g., embedding size, hidden layer size) could further enhance performance.\n",
    "\n",
    "#### Future Work:\n",
    "* Implement a validation set to monitor overfitting during training.\n",
    "* Explore alternative tokenization methods or pre-trained embeddings for better text representation.\n",
    "* Test the model on larger and more realistic datasets to evaluate its scalability and robustness."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
