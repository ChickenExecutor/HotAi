{"metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}}, "nbformat": 4, "nbformat_minor": 2, "cells": [{"cell_type": "markdown", "metadata": {"collapsed": false}, "source": "# NGram Neural Language Model - Exercises\n\nIn this exercise, we will set up a neural ngram language model and prepare corresponding training and test data based on a short sample text.\nWe will train the model on our very small corpus and perform inference."}, {"cell_type": "markdown", "metadata": {"collapsed": false}, "source": "### Package Setup\n\nIf not already done, please install the following packages to your python interpreter. (Remove the leading comment symbols `#` to execute the installation commands.)"}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2025-11-25T09:27:09.884521Z", "start_time": "2025-11-25T09:27:09.880280Z"}}, "outputs": [], "source": "#!pip install numpy==1.26.4\n#!pip install torch==2.2.2\n#!pip install torchtext==0.17.2\n#!pip install matplotlib\n#!pip install tensorboard"}, {"cell_type": "markdown", "metadata": {"collapsed": false}, "source": "### Package Imports\n\nExecute the follwoing cell to load all required packages to your running interpreter."}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2025-11-25T09:27:15.035524Z", "start_time": "2025-11-25T09:27:09.885533Z"}}, "outputs": [], "source": "import os\nfrom typing import Dict, List\n\nimport torch\nimport torchtext\nfrom matplotlib import pyplot as plt"}, {"cell_type": "markdown", "metadata": {"collapsed": false}, "source": "## Preparation of training data\n\nBefore we can start to implement and train a language model, the training data needs to be considered and preprocessed adequately.\n\nIn this tutorial, we will be working with a small sample text taken from a newspaper article from October 2024.\nThe sample text can be found in this notebook's directory's sub folder `data`.\nWe suggest to manually inspect the text file to get an idea of its contents.\n\nThe following code lines define the relative paths to the source file. \nDepending on your environment and IDE, you might need to adjust the paths according to your current working directory. "}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2025-11-25T14:23:22.476739Z", "start_time": "2025-11-25T14:23:22.455221Z"}, "collapsed": false}, "outputs": [], "source": "from data_processing import data_dir\ntrain_text_file_path = os.path.join(data_dir, \"sample_text.txt\")\n\nassert os.path.isfile(train_text_file_path)"}, {"cell_type": "markdown", "metadata": {"collapsed": false}, "source": "If the previous cell produced an error in the final assert statement, please check your current working directory with the subsequent code cell. \nAdjust the file paths above accordingly (so they contain the data file paths relative to your current directory). "}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2025-11-25T09:27:15.049641Z", "start_time": "2025-11-25T09:27:15.042337Z"}, "collapsed": false}, "outputs": [], "source": "os.getcwd()"}, {"cell_type": "markdown", "metadata": {"collapsed": false}, "source": "Next, we will define a function to read the text contents of our data file and read the file contents into a single string variable."}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2025-11-25T09:27:15.064646Z", "start_time": "2025-11-25T09:27:15.050647Z"}}, "outputs": [], "source": "def read_text_file(text_file_path: str) -> List[str]:\n    with open(text_file_path, \"r\", encoding=\"utf-8\") as file:\n        raw_text = file.read()\n    return raw_text\n\nraw_text = read_text_file(train_text_file_path)"}, {"cell_type": "markdown", "metadata": {"collapsed": false}, "source": "To ensure the text was read properly and get an idea what the text looks like, let's print the first 500 characters of the read text: "}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2025-11-25T09:27:15.070247Z", "start_time": "2025-11-25T09:27:15.065854Z"}, "collapsed": false}, "outputs": [], "source": "raw_text[:500]"}, {"cell_type": "markdown", "metadata": {"collapsed": false}, "source": "In the next step, the text needs to be tokenized, i.e. split into small units like words (or subwords).\n\nThe most naive approach would be to split the text at every whitespace and use each word as one token.\n\nThe following code performs such a naive tokenization and assembles a sorted list of all extracted tokens.\nLooking at a small excerpt of this list already reveals some problems of this naive approach.\nWhat are they?"}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2025-11-25T09:27:15.077091Z", "start_time": "2025-11-25T09:27:15.071253Z"}, "collapsed": false}, "outputs": [], "source": "tokens_naive = raw_text.split()\ntokens_naive_set = set().union(tokens_naive)\ntokens_naive_sorted = sorted(tokens_naive_set)\nsorted(tokens_naive_set)[36:40]"}, {"cell_type": "markdown", "metadata": {"collapsed": false}, "source": "The package torchtext provides out-of-the-box tokenizers for different languages and cases.\nWe will use the tokenizer for basic english language and employ it to tokenize our sentences.\nThis tokenizer already handles important issues like letter casing, punctuation characters, or quotation marks.  "}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2025-11-25T09:27:15.083481Z", "start_time": "2025-11-25T09:27:15.078097Z"}, "collapsed": false}, "outputs": [], "source": "tokenizer = torchtext.data.utils.get_tokenizer('basic_english')"}, {"cell_type": "markdown", "metadata": {"collapsed": false}, "source": "The tokenizer can be applied to a text, thereby converting it to a list of tokens."}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2025-11-25T09:27:15.089793Z", "start_time": "2025-11-25T09:27:15.084494Z"}}, "outputs": [], "source": "tokenized_text = tokenizer(raw_text)"}, {"cell_type": "markdown", "metadata": {"collapsed": false}, "source": "If we compare the numbers of unique tokens of the naive tokenization via the torchtext-provided tokenization, we will see that the number of unique tokens has indeed decreased."}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2025-11-25T09:27:15.096948Z", "start_time": "2025-11-25T09:27:15.090799Z"}, "collapsed": false}, "outputs": [], "source": "print(len(tokens_naive_sorted))\nprint(len(sorted(set().union(tokenized_text))))"}, {"cell_type": "markdown", "metadata": {"collapsed": false}, "source": "In this tutorial/demonstration case, we will split the data into a training and a test set in a very simple way:\nThe first 80% of the token sequence will be used for training, and the remaining 20% for test purposes.\n\n(In real-world scenarios, a more sophisticated split, depending on the data structure, should be applied.\nIn most cases, a threefold split into train-validation-test is advisable.)  "}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2025-11-25T09:27:15.102947Z", "start_time": "2025-11-25T09:27:15.097953Z"}, "collapsed": false}, "outputs": [], "source": "train_amount = int(0.8 * len(tokenized_text))\ntokenized_text_train = tokenized_text[:train_amount]\ntokenized_text_test = tokenized_text[train_amount:]\n\nprint(\"Token amount: Train {} / Test {}\".format(len(tokenized_text_train), len(tokenized_text_test)))"}, {"cell_type": "markdown", "metadata": {"collapsed": false}, "source": "## Vocabulary Definition\n\nIn the following, we will use the training data to define the vocabulary and the procedure for transforming token to numeric representations. \n\nFirst, we will manually build the vocabulary based on the sequence of tokens.\n\nTo do so, we first define a function which extracts all unique tokens and counts their number of appearances.\nThus, the function `get_vocabulary_and_frequencies(token_sequence)` receives a sequence of tokens and returns a dictionary.\nEach of the dictionary's `str`-keys corresponds to a unique token, the corresponding value counts how often the token appears within the given sequence.\n\nImplement the function accordingly."}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2025-11-25T09:27:15.109958Z", "start_time": "2025-11-25T09:27:15.103953Z"}}, "outputs": [], "source": "def get_vocabulary_and_frequencies(token_sequence: List[str]) -> Dict[str, int]:\n    vocab = {}\n    ### YOUR SOLUTION HERE\n    ### END OF SOLUTION\n    print(f\"Created vocabulary of {len(vocab)} unique tokens.\")\n    return vocab"}, {"cell_type": "markdown", "metadata": {"collapsed": false}, "source": "Execute and test the function."}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2025-11-25T09:27:15.116936Z", "start_time": "2025-11-25T09:27:15.110462Z"}}, "outputs": [], "source": "vocab = get_vocabulary_and_frequencies(tokenized_text_train)\n\nassert len(vocab) == 484\nassert vocab[\"\\'\"] == 17\nassert vocab[\"biden\"] == 6"}, {"cell_type": "markdown", "metadata": {"collapsed": false}, "source": "Next, we will use the created token-frequency-dictionary to create dictionaries for encoding and decoding tokens to numerics and vice versa. \n\nTo do so, we implement the function `get_encoding_decoding_dicts(vocabulary)`.\nIt receives the token-frequency-dictionary as input and produces two dictionaries:\n\n* One dictionary for encoding: The keys are the unique tokens in the vocabulary, the values are unique integers assigned to each token.\n* One dictionary for decoding: It reverses the key-item-relations of the encoding dictionary.\n\nWhen building the encoding dictionary for the `n` tokens within the vocabulary, choose the integers from `0` to `n-1` as values. \n\nAdditionally, the token `\"<unk>\"` will be added to the vocabulary. \nIt will be used as default token in case of unknown tokens (e.g. words not contained in the training data). "}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2025-11-25T09:27:15.123589Z", "start_time": "2025-11-25T09:27:15.117941Z"}}, "outputs": [], "source": "def get_encoding_decoding_dicts(vocabulary: Dict[str, int]):\n    vocab_encoding = {}\n    vocab_decoding = {}\n    ### YOUR SOLUTION HERE\n    ### END OF SOLUTION\n    unk_index = len(vocab_encoding)\n    vocab_encoding[\"<unk>\"] = unk_index\n    vocab_decoding[unk_index] = \"<unk>\"\n    return vocab_encoding, vocab_decoding"}, {"cell_type": "markdown", "metadata": {"collapsed": false}, "source": "Now, we can call the function to build the encoding and decoding dictionaries."}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2025-11-25T09:27:15.129031Z", "start_time": "2025-11-25T09:27:15.124594Z"}, "collapsed": false}, "outputs": [], "source": "encoding_vocab, decoding_vocab = get_encoding_decoding_dicts(vocab)\n\nassert len(encoding_vocab) == len(decoding_vocab)\nassert \"biden\" == decoding_vocab[encoding_vocab[\"biden\"]]\nassert \"\\'\" == decoding_vocab[encoding_vocab[\"\\'\"]]\nassert 123 == encoding_vocab[decoding_vocab[123]]"}, {"cell_type": "markdown", "metadata": {"collapsed": false}, "source": "The encoding dictionary can be used to convert the tokenized training text to numerical values, which can be used as input to our language model.\n\nImplement the function `encode_token_sequence(tokenized_sequence: List[str], encoding_dict:Dict[str, int], default_token = \"<unk>\")` which encodes a sequence of tokens based on the given dictionary.\nWhenever the sequence contains a token which is not contained in the encoding dictionary, use the default token instead."}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2025-11-25T09:27:15.136217Z", "start_time": "2025-11-25T09:27:15.130037Z"}}, "outputs": [], "source": "def encode_token_sequence(tokenized_sequence: List[str], encoding_dict:Dict[str, int], default_token = \"<unk>\"):\n    # query default token encoding once\n    default_token_encoded = encoding_dict[default_token]\n    \n    encoded_sequence = []\n    # iterate through token sequence and encode tokens one by one\n    ### YOUR SOLUTION HERE\n    ### END OF SOLUTION\n    return encoded_sequence"}, {"cell_type": "markdown", "metadata": {"collapsed": false}, "source": "We apply the just implemented function `encode_token_sequence` and the encoding dictionary to encode our training and test token sequences to numerical arrays."}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2025-11-25T09:27:15.142551Z", "start_time": "2025-11-25T09:27:15.137221Z"}, "collapsed": false}, "outputs": [], "source": "encoded_data_train = encode_token_sequence(tokenized_text_train, encoding_vocab, default_token = \"<unk>\")\nencoded_data_test = encode_token_sequence(tokenized_text_test, encoding_vocab, default_token = \"<unk>\")\n\nassert len(encoded_data_train) == len(tokenized_text_train)\nassert len(encoded_data_test) == len(tokenized_text_test)"}, {"cell_type": "markdown", "metadata": {"collapsed": false}, "source": "Next, we build our own PyTorch-compatible `Dataset`-class `NGramDataset` for n-gram text data.\n\nIn general, a `Dataset` class has access to a complete data set and makes its samples accessible to other PyTorch classes and functions. \n\nTo be able to use custom organized data with PyTorch and its training functionalities, one can implement a subclass of `torch.utils.data.Dataset`.\nThereby, it is mandatory to implement class initializer `__init__` and the methods `__getitem__` and `__len__`.\n\nIn our case, the class initializer takes the tensor containing the complete encoded corpus and the n-gram size.\nBoth are stored within the class as member variables and are then easily accessible within other class methods.\n\nThe method `__getitem__` receives an integer index as input and returns the corresponding data sample, i.e. the corresponding `x`- and `y`-values.\nIn case of n-gram data, the `x` (input) value are `n-1` coherent entries in the data, and the corresponding `y` (target) value is the next subsequent entry.\nMoreover, for `n=4`, the call of `__getitem__()` with `index=0` would return a tuple of a list of the first three encoded tokens in the data as first value, and the third encoded token as second value.\n\nThe method `__len__` has no arguments and returns the number of samples within the dataset.\n\nImplement the two methods below.\n"}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2025-11-25T09:27:15.149545Z", "start_time": "2025-11-25T09:27:15.143557Z"}}, "outputs": [], "source": "class NGramDataset(torch.utils.data.Dataset):\n\n    def __init__(self, corpus, ngram_size):\n        super(NGramDataset).__init__()\n        self.ngram_size = ngram_size\n        self.complete_corpus = torch.tensor(corpus)\n\n\n    def __getitem__(self, index):\n        ### YOUR SOLUTION HERE\n        ### END OF SOLUTION\n    \n\n    def __len__(self):\n        ### YOUR SOLUTION HERE\n        ### END OF SOLUTION"}, {"cell_type": "markdown", "metadata": {"collapsed": false}, "source": "Now that the dataset class is defined, we pour our training and test data into it.\n\nFurther, we choose `n=5`."}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2025-11-25T09:27:15.157549Z", "start_time": "2025-11-25T09:27:15.150550Z"}}, "outputs": [], "source": "ngram_length = 5\n\ndataset_train = NGramDataset(encoded_data_train, ngram_length)\ndataset_test = NGramDataset(encoded_data_test, ngram_length)\n\nassert len(dataset_train) == 1200\nassert len(dataset_test) == 297"}, {"cell_type": "markdown", "metadata": {"collapsed": false}, "source": "Next, we use PyTorch's `DataLoader` to provide batches of data for the training and evaluation process.\nAlso, shuffling of training data with each epoch is already implemented in this class.\n\nWhen training and evaluation our model, we will be able to iterate over these dataloader"}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2025-11-25T09:27:15.162770Z", "start_time": "2025-11-25T09:27:15.157549Z"}}, "outputs": [], "source": "dl_train = torch.utils.data.DataLoader(dataset_train, shuffle=True, batch_size=32)\ndl_test = torch.utils.data.DataLoader(dataset_test, batch_size=32)"}, {"cell_type": "markdown", "metadata": {"collapsed": false}, "source": "## Model Definition and Implementation\n\nNow, we define our neural n-gram model as a subclass of `torch.nn.Module` (compare to previous exercises).\n\nOur neural n-gram model consists of an embedding-layer, which projects the encoded input tokens in their numeric represantation to a multidimensional embedding space.\nNext, a fully-connected layer with ReLU activation and another fully-connected layer follow.\nThe latter fully connected layer's output dimension corresponds to the number of unique tokens in the vocabulary with each unit giving a score for the corresponding token in the vocabulary.\n\nIn the following, the class initializer, which initialize all the required neural network components is already implemented.\nImplement the class' `forward()`-method which computes the model's forward pass.\nThe input to the `forward()`-function can be assumed to be of shape `(batch_size, n-1)`.\n\nNote that the embeddings layer produces a 3-dimensional tensor of shape `(batch_size, n-1, embedding_dim)`.\nThis tensor needs to be flattened to 2 dimensions (`(batch_size, (n-1) * embedding_dim)`) before it can be input into the first linear layer."}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2025-11-25T09:27:15.169220Z", "start_time": "2025-11-25T09:27:15.163780Z"}}, "outputs": [], "source": "class NeuralNgram(torch.nn.Module):\n\n    def __init__(self, ngram_size: int, vocab_size: int, embedding_dim: int = 64, hidden_dim: int = 128):\n        super(NeuralNgram, self).__init__()\n        self.ngram_size = ngram_size\n        self.embeddings = torch.nn.Embedding(vocab_size, embedding_dim)\n        self.linear_1 = torch.nn.Linear((ngram_size - 1) * embedding_dim, hidden_dim)\n        self.activation = torch.nn.ReLU()\n        self.linear_2 = torch.nn.Linear(hidden_dim, vocab_size)\n\n    def forward(self, x):\n        ### YOUR SOLUTION HERE\n        ### END OF SOLUTION"}, {"cell_type": "markdown", "metadata": {"collapsed": false}, "source": "A simple test for your implementation follows:"}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2025-11-25T09:27:15.188135Z", "start_time": "2025-11-25T09:27:15.170236Z"}, "collapsed": false}, "outputs": [], "source": "model = NeuralNgram(ngram_length, len(encoding_vocab))\n\ntest_tensor = torch.zeros((32, ngram_length - 1)).long()\nmodel_out = model(test_tensor)\n\nassert model_out.shape[0] == 32\nassert model_out.shape[1] == len(encoding_vocab)"}, {"cell_type": "markdown", "metadata": {"collapsed": false}, "source": "We instantiate our model as instance of the class `NeuralNgram`.\nWhen initializing the class, `n=5` is set according to the choice above and the number of unique tokens in our vocabulary is passed as input parameter.\n\nFurther, we initialize the Cross Entropy Loss and a stochastic gradient descent optimizer for the model's parameters."}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2025-11-25T09:27:16.042247Z", "start_time": "2025-11-25T09:27:15.189149Z"}}, "outputs": [], "source": "model = NeuralNgram(ngram_length, len(encoding_vocab))\n\nloss = torch.nn.CrossEntropyLoss(reduction='sum')\noptimizer = torch.optim.SGD(lr=0.01, params=model.parameters())"}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2025-11-25T09:27:20.331188Z", "start_time": "2025-11-25T09:27:16.042247Z"}}, "outputs": [], "source": "n_epochs = 30\n\ntrain_losses = []\ntest_losses = []\n\nfor i in range(n_epochs):\n    # Perform training epoch\n    model.train()\n    epoch_loss = 0\n    for data_x, data_y in dl_train:\n        ### YOUR SOLUTION HERE\n        # 1) run model on data\n        # 2) compute loss (note: target values need to be converted to long before being passed to the cross-entropy-loss-function)\n        ### END OF SOLUTION\n        epoch_loss += batch_loss.cpu().detach().numpy()\n        optimizer.zero_grad()\n        batch_loss.backward()\n        optimizer.step()\n\n    epoch_loss = epoch_loss / len(dataset_train)\n    train_losses.append(epoch_loss)\n\n    # Evaluate on test data\n    model.eval()\n    test_loss = 0\n    for test_x, test_y in dl_test:\n        predictions = model(test_x)\n        batch_loss = loss(predictions, test_y.long())\n        test_loss += batch_loss.cpu().detach().numpy()\n\n    test_loss = test_loss / len(dataset_test)\n    test_losses.append(test_loss)\n\n    if i%5 == 0:\n        print(\"Epoch {}\".format(i+1))\n        print(\"Train loss: {}\".format(epoch_loss))\n        print(\"Test Loss: {}\".format(test_loss))"}, {"cell_type": "markdown", "metadata": {"collapsed": false}, "source": "Looking at the evolution of training and test loss, it becomes obvious that overfitting is an issue in our training pipeline.\n\nWhat do you think is the main reason for overfitting in this case?"}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2025-11-25T09:27:20.463634Z", "start_time": "2025-11-25T09:27:20.332203Z"}, "collapsed": false}, "outputs": [], "source": "plt.figure()\nplt.plot(train_losses, label=\"Train Loss\")\nplt.plot(test_losses, label=\"Test Loss\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.show()"}, {"cell_type": "markdown", "metadata": {"collapsed": false}, "source": "Next, we want to compute the model's perplexity on training and test data.\n\nTherefore, we compute the perplexity using the cross entropy loss: "}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2025-11-25T09:27:20.467863Z", "start_time": "2025-11-25T09:27:20.464640Z"}}, "outputs": [], "source": "def compute_perplexity(model, data_loader):\n    loss = torch.nn.CrossEntropyLoss(reduction=\"sum\")\n    loss_sum = 0\n    for data_x, data_y in data_loader:\n        scores = model(data_x)\n        loss_sum += loss(scores, data_y.long()) \n\n    perplexity = torch.exp(loss_sum / len(data_loader.dataset))\n    return perplexity"}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2025-11-25T09:27:20.520872Z", "start_time": "2025-11-25T09:27:20.468870Z"}}, "outputs": [], "source": "ppl_train = compute_perplexity(model, dl_train)\nprint(ppl_train)"}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2025-11-25T09:27:20.536620Z", "start_time": "2025-11-25T09:27:20.522193Z"}, "collapsed": false}, "outputs": [], "source": "ppl_test = compute_perplexity(model, dl_test)\nprint(ppl_test)"}, {"cell_type": "markdown", "metadata": {"collapsed": false}, "source": "Both loss values output during training and perplexity values again indicate that our model tremendously overfitted the training data."}, {"cell_type": "markdown", "metadata": {"collapsed": false}, "source": "Nevertheless, we want to implement inference on our neural n-gram model and have the model generate a few lines of text.\n\nThe function `inference(ngram_words, encoding_vocab, decoding_vocab, model)` takes as arguments:\n* A list of n-1 tokens which correspond to the beginning of an arbitrary n-gram.\n* Encoding and decoding vocabulary to convert tokens to numerical representations and vice versa\n* The trained n-gram model\n\nIt queries the model's predictions for the n-gram and extracts the token with the highest score, which is returned by the function. "}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2025-11-25T09:27:20.540985Z", "start_time": "2025-11-25T09:27:20.537629Z"}}, "outputs": [], "source": "def inference(ngram_words, encoding_vocab, decoding_vocab, model):\n    ngram_tokens = [encoding_vocab[token] for token in ngram_words]\n    token_tensor = torch.Tensor([ngram_tokens]).long()\n    ### YOUR SOLUTION HERE\n    # Execute model and find token (word) with the highest predicted score\n    ### END OF SOLUTION\n    return next_word"}, {"cell_type": "markdown", "metadata": {"collapsed": false}, "source": "Next, we define a starting n-gram (4 words from the dictionary) and let our model predict next words iteratively."}, {"cell_type": "code", "execution_count": null, "metadata": {"ExecuteTime": {"end_time": "2025-11-25T09:27:20.582381Z", "start_time": "2025-11-25T09:27:20.541992Z"}}, "outputs": [], "source": "start_ngram = [\"the\", \"presidential\", \"election\", \"was\"]\ncurrent_ngram = start_ngram\ncomplete_sequence = \" \".join(start_ngram) + \" \"\nfor i in range(100):\n    ### YOUR SOLUTION HERE\n    # have the model make a word prediction\n    # update current_ngram for the next iteration\n    # append the predicted word to the complete sequence (type str, human-readable)\n    ### END OF SOLUTION\nprint(complete_sequence)"}, {"cell_type": "markdown", "metadata": {"collapsed": false}, "source": "The model predictions reflect the very small amount of training data on which the model has tremendously overfitted.\nThat is, after few iterations, it starts repeating complete text passages from the training data.\n\nStill, compared to statistical n-gram models, it is able to process n-grams which were not previously seen in the training data (\"the presedential election was\" is not a sequence present in our sample text). "}, {"cell_type": "markdown", "metadata": {"collapsed": false}, "source": "## Summary of Results and Observations\n\n#### Model Performance:  \n* The training loss decreased steadily over epochs, indicating that the model successfully learned patterns from the training data.\n* However, the test loss and perplexity values suggest significant overfitting due to the small dataset size.\n\n#### Overfitting:  \n* The model memorized the training data, as evidenced by the repetitive text generation during inference.\n* This overfitting is likely caused by the limited training data and the absence of regularization techniques.\n\n#### Inference Results:  \n* The model was able to generate coherent sequences for n-grams not seen during training, demonstrating its ability to generalize to some extent.\n* However, the generated text quickly devolved into repetitive patterns, reflecting the overfitting issue.\n\n#### Key Takeaways:\n* A complete pipeline for implementing, training, and evaluating a simple neural n-gram model was demonstrated, showcasing the steps from data preprocessing to inference. \n* Increasing the dataset size or using a more diverse corpus would likely improve generalization.\n* Regularization techniques such as dropout, weight decay, or early stopping could help mitigate overfitting.\n* Experimenting with different model architectures or hyperparameters (e.g., embedding size, hidden layer size) could further enhance performance.\n\n#### Future Work:\n* Implement a validation set to monitor overfitting during training.\n* Explore alternative tokenization methods or pre-trained embeddings for better text representation.\n* Test the model on larger and more realistic datasets to evaluate its scalability and robustness."}]}