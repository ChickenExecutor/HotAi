{"metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}}, "nbformat": 4, "nbformat_minor": 5, "cells": [{"cell_type": "markdown", "id": "0", "metadata": {"collapsed": false}, "source": "# LSTM Language Model - Inference\n\nIn this tutorial, we will focus on a LSTM language model trained on the open source dataset wikitext-103.\nThe dataset consists of verified wikipedia articles and contains more than 100 million tokens.\n\nAs the complexity of dataset and model do not allow for a quick model training on commonly available hardware resources, we will consider only the set-up of data and models, and work with a pretrained model. \nThe training was performed on a single GPU unit for 12 epochs.\n \nWe will see that the resulting model incorporates some text generation abilities but certainly lacks any language proficiency.  "}, {"cell_type": "code", "execution_count": null, "id": "1", "metadata": {"ExecuteTime": {"end_time": "2025-11-25T10:06:49.753731Z", "start_time": "2025-11-25T10:06:49.749730Z"}, "collapsed": false}, "outputs": [], "source": "import os\n\nimport torch\nimport torchtext\nfrom matplotlib import pyplot as plt\nfrom sklearn.decomposition import PCA"}, {"cell_type": "markdown", "id": "2", "metadata": {"collapsed": false}, "source": "## The Dataset\n\nThe attached file `data_processing.py` contains functions for loading and processing the data of the **wikitext-103** dataset, which consists of verified Wikipedia articles.\n\nMore information on the dataset [is available here](https://www.kaggle.com/datasets/vadimkurochkin/wikitext-103).\n\nThe data itself can be found in this directory's subfolder `data/wikitext-103`.\nIt consists of three data files: `wiki.train.tokens`, `wiki.valid.tokens` and `wiki.test.tokens`, containing train, validation and test data, respectively.\n\nAs in the previous exercise, the basic english tokenizer as provided by `torchtext` was used for text tokenization.\n\nWe load the vocabulary built from the dataset's training split as instance of torchtext's class `Vocab`.\n\nAs the training split is very large, we do not load it at this point. \nInstead, we use the function `load_dataset_from_file` to read the test data file, encode it according to tokenizer and vocabulary and load it to a PyTorch-compatible data class."}, {"cell_type": "code", "execution_count": null, "id": "3", "metadata": {"ExecuteTime": {"end_time": "2025-11-25T10:06:49.859187Z", "start_time": "2025-11-25T10:06:49.779661Z"}, "collapsed": true}, "outputs": [], "source": "from data_processing import load_vocab, load_dataset_from_file, data_dir_wikitext_103 as data_dir\ntokenizer = torchtext.data.utils.get_tokenizer('basic_english')\nvocab = load_vocab(os.path.join(data_dir, \"vocab.pkl\"))"}, {"cell_type": "markdown", "id": "4", "metadata": {"collapsed": false}, "source": "We can query the size of the vocabulary as"}, {"cell_type": "code", "execution_count": null, "id": "5", "metadata": {"ExecuteTime": {"end_time": "2025-11-25T10:06:49.864442Z", "start_time": "2025-11-25T10:06:49.860192Z"}, "collapsed": false}, "outputs": [], "source": "len(vocab)"}, {"cell_type": "markdown", "id": "6", "metadata": {"collapsed": false}, "source": "A token (word) can be encoded as"}, {"cell_type": "code", "execution_count": null, "id": "7", "metadata": {"ExecuteTime": {"end_time": "2025-11-25T10:06:49.873943Z", "start_time": "2025-11-25T10:06:49.865448Z"}, "collapsed": false}, "outputs": [], "source": "vocab[\"something\"]"}, {"cell_type": "markdown", "id": "8", "metadata": {"collapsed": false}, "source": "and decoded as"}, {"cell_type": "code", "execution_count": null, "id": "9", "metadata": {"ExecuteTime": {"end_time": "2025-11-25T10:06:49.892459Z", "start_time": "2025-11-25T10:06:49.875949Z"}, "collapsed": false}, "outputs": [], "source": "vocab.get_itos()[1001]"}, {"cell_type": "markdown", "id": "10", "metadata": {"collapsed": false}, "source": "The data is organized in sentences where each sentence is started with a dedicated start (`<sos>`) and ended with a dedicated end (`<eos>`) token.\nEach sentence is padded to identical length, so that arbitrary sequences can be batched in a single tensor. \nTherefore, a pad token (`<pad>`) is used. \nThe token indicates the end of the sequence has been reached, and it is explicitly ignored on model training and evaluation (when computing losses, gradients and metrics). \n\nEach data sample contains the complete encoded sequence as x-value.\nThe corresponding y-value (target) is the same sequence shifted to the left.\nThis is as in every step, one token of the input is fed into the recurrent LSTM model, and the next sequence token is the model's expected output. "}, {"cell_type": "code", "execution_count": null, "id": "11", "metadata": {"ExecuteTime": {"end_time": "2025-11-25T10:07:48.335744Z", "start_time": "2025-11-25T10:07:48.230121Z"}, "collapsed": false}, "outputs": [], "source": "# The following code only works if the wikitext-103 dataset is available in the specified data_dir.\n# Otherwise, this cell can be ignored. The rest of the notebook works without loading the dataset.\ndataset_test = load_dataset_from_file(os.path.join(data_dir, \"wiki.test.tokens\"), tokenizer, vocab, max_seq_length=500, pad_token_index=vocab[\"<pad>\"])\nsample_x, sample_y = dataset_test[0]\nassert torch.all(sample_x[1:] == sample_y[0:-1])"}, {"cell_type": "markdown", "id": "12", "metadata": {"collapsed": false}, "source": "## Restoring the pretrained model\n\nTo be able to load the pretrained model weights, the identical model architecture needs to be constructed first. \n\nThe following class, `LM_LSTM_Model` contains the model architecture of our pretrained model.\n\nInspect the architecture and notice the use of the LSTM-class provided by `PyTorch`.\nEspecially note that the `forward()`-method takes an additional argument: `hidden`. \nThis argument contains the networks hidden states (both hidden and context layers) after processing of the previous sequence element.\n\nNote that the hidden state has to be set to zero before processing a new sequence.\nTo initialize new hidden states, the method `init_hidden` is provided."}, {"cell_type": "code", "execution_count": null, "id": "13", "metadata": {"ExecuteTime": {"end_time": "2025-11-25T10:06:50.007776Z", "start_time": "2025-11-25T10:06:50.003598Z"}, "collapsed": false}, "outputs": [], "source": "class LM_LSTM_Model(torch.nn.Module):\n    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, dropout_rate):\n        super(LM_LSTM_Model, self).__init__()\n        self.num_layers = num_layers\n        self.hidden_dim = hidden_dim\n        self.embedding_dim = embedding_dim\n        embedding = torch.nn.Embedding(vocab_size, embedding_dim)\n        self.embedding = embedding\n        self.lstm = torch.nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers,\n                                  dropout=dropout_rate, batch_first=True)\n        self.dropout = torch.nn.Dropout(dropout_rate)\n        self.fc = torch.nn.Linear(hidden_dim, vocab_size)\n\n    def forward(self, x, hidden):\n        embedding = self.dropout(self.embedding(x))\n        output, hidden = self.lstm(embedding, hidden)\n        output = self.dropout(output)\n        prediction = self.fc(output)\n        return prediction, hidden\n\n    def init_hidden(self, batch_size, device=\"cpu\"):\n        hidden = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device)\n        cell = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device)\n        return hidden, cell"}, {"cell_type": "markdown", "id": "14", "metadata": {"collapsed": false}, "source": "We initialize the model (using the same parameters as in training) and set it to evaluation mode.\n(In evaluation mode, training functionalities are disabled, e.g. gradient computation is not activated and dropout is not applied.)"}, {"cell_type": "code", "execution_count": null, "id": "15", "metadata": {"ExecuteTime": {"end_time": "2025-11-25T10:06:51.135199Z", "start_time": "2025-11-25T10:06:50.008782Z"}, "collapsed": false}, "outputs": [], "source": "embedding_dim = 512\nhidden_dim = 1024\nnum_layers = 3\n\nmodel = LM_LSTM_Model(len(vocab), embedding_dim, hidden_dim, num_layers, 0)\nmodel.eval()"}, {"cell_type": "markdown", "id": "16", "metadata": {"collapsed": false}, "source": "Next, we load the model weights thereby restoring the model trained on our dataset."}, {"cell_type": "code", "execution_count": null, "id": "17", "metadata": {"ExecuteTime": {"end_time": "2025-11-25T10:06:51.774474Z", "start_time": "2025-11-25T10:06:51.136205Z"}, "collapsed": false}, "outputs": [], "source": "state_dict = torch.load(f\"./pretrained_model/lstm_model_weights.pkl\")\nmodel.load_state_dict(state_dict)"}, {"cell_type": "markdown", "id": "18", "metadata": {"collapsed": false}, "source": "The plots of the model's training and evaluation loss over the trained epochs show that the LSTM model at hand does not suffer from excessive overfitting as seen for the neural ngram model previously.\n\n![image](images/lstm_model_loss_train.png) ![image](images/lstm_model_loss_eval.png)"}, {"cell_type": "markdown", "id": "19", "metadata": {"collapsed": false}, "source": "We want to use the pretrained model to create some texts.\n\nTherefore, we implement the function `generate_text(model, vocab, start_tokens, max_iterations)` which takes the model and its vocabulary as first two input parameters.\nFurther, the start of a sequence which the model will be continuing is input as list of strings in `start_tokens`.\nThe parameter `max_iterations` contains the maximum number of additional tokens to be generated by the model.\n\nComplete the function according to the instructions in the line comments."}, {"cell_type": "code", "execution_count": null, "id": "20", "metadata": {"ExecuteTime": {"end_time": "2025-11-25T10:06:51.780895Z", "start_time": "2025-11-25T10:06:51.775607Z"}, "collapsed": false}, "outputs": [], "source": "def generate_text(model, vocab, start_tokens, max_iterations):\n    model.eval()\n    \n    # encode the start sequence using the vocabulary\n    start_tokens_encoded = vocab(start_tokens)\n    \n    # initialize variable to hold the complete generated sequence as encoded tokens (numerical) (needs to be concatenated to as generation proceeds) \n    complete_sequence = torch.tensor(start_tokens_encoded)\n\n    # Initialize the LSTM's hidden units for the new sequence\n    hidden = model.init_hidden(1)\n    ### YOUR SOLUTION HERE\n    # Create a tensor containing the first tokens (start_tokens) and run them through the network to obtain the corresponding hidden state\n    ### END OF SOLUTION\n    \n    # Iteratively create next tokens and append them to the complete sequence.\n    # Stop if \"<eos>\" is predicted or when the maximum number of iterations is reached.\n    for i in range(max_iterations):\n        ### YOUR SOLUTION HERE\n        # 1) extract next model input, i.e. last token of complete sequence. (in each iteration, only the last token, which was not yet passed through the network, needs to be fed to the model.) \n        # 2) run model on next token (update hidden state variable)\n        # 3) mask the prediction for the default-token \"<unk>\" as we do not want our model to predict this token\n        # 4) find the token with the highest predicted score\n        ### END OF SOLUTION\n        \n        # add next predicted token to tensor containing the complete sequence\n        complete_sequence = torch.concat([complete_sequence, next_word_code.unsqueeze(0)], -1)\n\n        # stop, if \"<eos>\" is predicted\n        if next_word_code in vocab([\"<eos>\"]):\n            break\n\n    # decode complete sequence to human-readable tokens\n    result_tokens = [vocab.get_itos()[i] for i in complete_sequence]\n    result_sequence = \" \".join(result_tokens)\n    return result_sequence"}, {"cell_type": "markdown", "id": "21", "metadata": {"collapsed": false}, "source": "We can use the function above to test the pretrained language model."}, {"cell_type": "code", "execution_count": null, "id": "22", "metadata": {"ExecuteTime": {"end_time": "2025-11-25T10:06:54.435354Z", "start_time": "2025-11-25T10:06:51.781904Z"}, "collapsed": false}, "outputs": [], "source": "sequence_start = tokenizer(\"<sos> the quick brown fox\")\ngenerate_text(model, vocab, sequence_start, 200)"}, {"cell_type": "markdown", "id": "23", "metadata": {"collapsed": false}, "source": "## Model Embeddings\n\nAs seen in the previous exercise, we can visualize the embeddings inherent in the model (learned simultaneously with the language model as a whole).\n\nBelow, we perform principal component analysis (PCA) to reduce the dimensionality of the embeddings to two dimensions for easier plotting. \nWe create a scatter plot to visualize the spatial relationships between selected words.\n\nConsidering the arrangement of word embeddings in space, what can you say about the learned embeddings? How do they compare to the sophisticated GloVe embeddings inspected in the previous exercise session?  "}, {"cell_type": "code", "execution_count": null, "id": "24", "metadata": {"ExecuteTime": {"end_time": "2025-11-25T10:06:54.439638Z", "start_time": "2025-11-25T10:06:54.436359Z"}, "collapsed": false}, "outputs": [], "source": "def get_token_embedding(token: str, vocab, model):\n    encoded_token = vocab[token]\n    model_input = torch.tensor([encoded_token])\n    embedding = model.embedding(model_input)\n    return embedding"}, {"cell_type": "code", "execution_count": null, "id": "25", "metadata": {"ExecuteTime": {"end_time": "2025-11-25T10:06:54.457942Z", "start_time": "2025-11-25T10:06:54.440643Z"}, "collapsed": false}, "outputs": [], "source": "def plot_word_embeddings(words, embeddings):\n    word_vectors = [embeddings[word] for word in words if word in embeddings]\n    pca = PCA(n_components=2)\n    word_vectors_2d = pca.fit_transform(word_vectors)\n    \n    plt.figure(figsize=(10, 10))\n    for i, word in enumerate(words):\n        if word in embeddings:\n            plt.scatter(word_vectors_2d[i, 0], word_vectors_2d[i, 1])\n            plt.text(word_vectors_2d[i, 0] + 0.01, word_vectors_2d[i, 1] + 0.01, word, fontsize=9)\n    plt.show()"}, {"cell_type": "code", "execution_count": null, "id": "26", "metadata": {"ExecuteTime": {"end_time": "2025-11-25T10:06:54.465838Z", "start_time": "2025-11-25T10:06:54.458947Z"}, "collapsed": false}, "outputs": [], "source": "words = [\"king\", \"queen\", \"mother\", \"father\", \"soda\", \"coke\", \"france\", \"spain\"]\nembeddings = {token: get_token_embedding(token, vocab, model).squeeze(0).detach().numpy() for token in words}"}, {"cell_type": "code", "execution_count": null, "id": "27", "metadata": {"ExecuteTime": {"end_time": "2025-11-25T10:06:54.549121Z", "start_time": "2025-11-25T10:06:54.466851Z"}, "collapsed": false}, "outputs": [], "source": "plot_word_embeddings(words, embeddings)"}]}