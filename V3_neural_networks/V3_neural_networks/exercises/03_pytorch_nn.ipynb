{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {
    "collapsed": false,
    "nbgrader": {
     "grade_id": "cell-6a21ab8a6cc9",
     "locked": true,
     "schema_version": 3
    }
   },
   "source": [
    "# Neural Networks with PyTorch - Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T08:52:08.087079Z",
     "start_time": "2024-11-06T08:52:04.005296Z"
    },
    "collapsed": true,
    "nbgrader": {
     "grade_id": "cell-a9f3c1a7ae38",
     "locked": true,
     "schema_version": 3
    }
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {
    "collapsed": false,
    "nbgrader": {
     "grade_id": "cell-9d53a9ffb756",
     "locked": true,
     "schema_version": 3
    }
   },
   "source": [
    "### Tensors\n",
    "\n",
    "PyTorch tensors are multidimensional data containers, conceptually identical to numpy arrays\n",
    "(in many cases, identical functions exist in both frameworks for the respective data arrays).\n",
    "\n",
    "Use the [PyTorch Documentation](https://pytorch.org/docs/stable/tensors.html) to initialize tensors:\n",
    "* 1-D Tensor of size 5 containing the natural numbers 1,2,3,4,5\n",
    "* 2-D Tensor of size [3,3] containing random float numbers\n",
    "* 3-D Tensor of size [1, 1, 1] containing data of type unsigned integer (torch.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T08:52:08.102744Z",
     "start_time": "2024-11-06T08:52:08.088093Z"
    },
    "collapsed": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-ba64e4b48080",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[4]]], dtype=torch.uint8)\n"
     ]
    }
   ],
   "source": [
    "### YOUR SOLUTION HERE\n",
    "d1 = torch.tensor([1,2,3,4,5])\n",
    "d2 = torch.randn(3,3)\n",
    "d3 = torch.randint(0, 256, [1, 1, 1], dtype=torch.uint8)\n",
    "print(d3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {
    "collapsed": false,
    "nbgrader": {
     "grade_id": "cell-e3936cffeb1b",
     "locked": true,
     "schema_version": 3
    }
   },
   "source": [
    "What happens if you sum (multiply) the previously created tensors of shape (5) and (1,1,1)?\n",
    "Try it and assign the results to the variable `` tensor_sum`` (``tensor_product``)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T08:52:08.116462Z",
     "start_time": "2024-11-06T08:52:08.103757Z"
    },
    "collapsed": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-08c4e8f8fb86",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "### YOUR SOLUTION HERE\n",
    "tensor_sum = d1+d3\n",
    "tensor_product = d1*d3\n",
    "### END OF SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {
    "collapsed": false,
    "nbgrader": {
     "grade_id": "cell-4aeef573f2ee",
     "locked": true,
     "schema_version": 3
    }
   },
   "source": [
    "Have a look at these basic [tensor operations](https://pytorch.org/docs/stable/torch.html#indexing-slicing-joining-mutating-ops).\n",
    "\n",
    "* Create a tensor of shape (10) by concatenating two versions of the previously created 5-entry tensor. \n",
    "* Create a tensor of shape (9) from the previously created random tensor of shape (3, 3).\n",
    "* Create the transposed version of your random tensor of shape (3, 3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T08:52:08.129060Z",
     "start_time": "2024-11-06T08:52:08.117480Z"
    },
    "collapsed": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-3f942d067052",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.4403, -2.2917,  2.2799],\n",
      "        [-0.9781,  1.9756, -0.0719],\n",
      "        [-1.4433, -0.1874, -0.4242]])\n"
     ]
    }
   ],
   "source": [
    "### YOUR SOLUTION HERE\n",
    "t10 = torch.cat((d1,d1))\n",
    "t9 = torch.flatten(d2)\n",
    "t2_t = torch.transpose(d2, 0, 1)\n",
    "print(t2_t)\n",
    "### END OF SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {
    "collapsed": false,
    "nbgrader": {
     "grade_id": "cell-c129b5c19fdb",
     "locked": true,
     "schema_version": 3
    }
   },
   "source": [
    "### Computing a neuron with pyTorch\n",
    "\n",
    "Performing the computations for a single Neuron with pyTorch and tensors is easy!\n",
    "The required weighted sum can be computed by using built-in vector products,\n",
    "and different activation functions are also readily available.\n",
    "\n",
    "In the following, define a tensor ``weights`` of shape (5) which contains the weights of the neuron to be computed.\n",
    "Further, instantiate the class [``torch.nn.Sigmoid()``](https://pytorch.org/docs/stable/generated/torch.nn.Sigmoid.html#sigmoid) in a variable ``activation``,\n",
    "which will be used to compute the neuron's activation value.\n",
    "\n",
    "Create a tensor variable ``input = torch.ones((5,1))`` incorporating the input values to the neuron.\n",
    "\n",
    "Then, compute the neuron's output according to the lecture. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T08:52:08.161217Z",
     "start_time": "2024-11-06T08:52:08.130068Z"
    },
    "collapsed": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-91993b32bd90",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.7278, 0.2782, 0.6759, 0.5987, 0.3448])\n"
     ]
    }
   ],
   "source": [
    "weights = torch.randn((5))\n",
    "activation = torch.nn.Sigmoid()\n",
    "\n",
    "input = torch.ones((5))\n",
    "\n",
    "### YOUR SOLUTION HERE\n",
    "# Compute Neuron output (weighted sum and sigmoid activation)\n",
    "output = activation(weights*input)\n",
    "### END OF SOLUTION\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {
    "collapsed": false,
    "nbgrader": {
     "grade_id": "cell-d9ef8a86bac9",
     "locked": true,
     "schema_version": 3
    }
   },
   "source": [
    "### Neural Network Components\n",
    "\n",
    "However, computations like these do not need to be implemented manually in most cases.\n",
    "PyTorch already contains classes and functions for most basic neural network layer types and components.\n",
    "\n",
    "In the following, we will implement a simple neural network.\n",
    "The network operates on input data of size 13.\n",
    "There is one hidden layer within the network consisting of 20 neurons (fully-connected).\n",
    "The network's output layer contains 3 neurons.\n",
    "\n",
    "Use the class [``torch.nn.Linear``](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) to create two fully-connected neural network layers.\n",
    "The class' constructor takes the number of input units and the number of output units as arguments.\n",
    "Further, the argument ``bias=True`` activates the bias for all neurons in the layer.\n",
    "The layer's weights are randomly initialized (using uniform Xavier initialization).\n",
    "\n",
    "For each of the layers, create an activation function.\n",
    "The first layer uses a Sigmoid activation function, \n",
    "for the second layer, employ an instance of the class [``torch.nn.Softmax``](https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html#softmax) as activation function.\n",
    "\n",
    "Last, define a function ``neural_net(input_tensor)`` which computes the neural network's output values for the given input tensor of shape (*, 13).\n",
    "(Note, that 2-dimensional input tensors are required as pyTorch's neural networks are designed to compute outputs for multiple samples (a whole batch) at once.) \n",
    "Using the function, compute the neural network's output for the input tensor ``input = torch.ones(1,3)``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T08:52:08.183323Z",
     "start_time": "2024-11-06T08:52:08.162228Z"
    },
    "collapsed": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-76bd51b7d66c",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\keanu\\UNI\\HotAi\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.3178, 0.4302, 0.2520]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize fully-connected layers and activation functions according to specifications above (dimensions, function types, etc.)\n",
    "### YOUR SOLUTION HERE\n",
    "fc1 = torch.nn.Linear(in_features=13, out_features=20, bias = True)\n",
    "fc2 = torch.nn.Linear(in_features=20, out_features=3, bias=True)\n",
    "softmax = torch.nn.Softmax()\n",
    "### END OF SOLUTION\n",
    "\n",
    "def neural_net(input_tensor):\n",
    "    # Compute neural network output (pass input through both layers and apply activation functions as defined above)\n",
    "    ### YOUR SOLUTION HERE\n",
    "    output = fc1(input_tensor)\n",
    "    output = fc2(output)\n",
    "    output = softmax(output)\n",
    "    return output\n",
    "    ### END OF SOLUTION\n",
    "\n",
    "input = torch.ones(1,13)\n",
    "neural_net(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {
    "collapsed": false,
    "nbgrader": {
     "grade_id": "cell-764fefd31a11",
     "locked": true,
     "schema_version": 3
    }
   },
   "source": [
    "### Simple Neural Network with pyTorch\n",
    "\n",
    "However, to be able to fully utilize the previously defined neural network inside pyTorch,\n",
    "we need to instantiate it inside a class derived from the base class ``torch.nn.Module``.\n",
    "\n",
    "Such a custom model class must implement the function ``forward(self, x)`` which computes the model's forward pass for an input tensor x.\n",
    "\n",
    "In the following, implement the simple neural network described above inside the class ``MyNeuralNetwork``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "13",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T08:52:08.187528Z",
     "start_time": "2024-11-06T08:52:08.184329Z"
    },
    "collapsed": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-f8e1302ba66b",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "class MyNeuralNetwork(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(MyNeuralNetwork, self).__init__()\n",
    "        ### YOUR SOLUTION HERE\n",
    "        self.fc1 = torch.nn.Linear(in_features=5, out_features=60, bias = True)\n",
    "        self.activation1 = torch.nn.ReLU()\n",
    "        self.fc2 = torch.nn.Linear(in_features=60, out_features=3, bias=True)\n",
    "        self.activation2 = torch.nn.Softmax(1)\n",
    "        ### END OF SOLUTION\n",
    "        \n",
    "    def forward(self, x):\n",
    "        ### YOUR SOLUTION HERE\n",
    "        x = self.fc1(x)\n",
    "        x = self.activation1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.activation2(x)\n",
    "        return x\n",
    "        ### END OF SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {
    "nbgrader": {
     "grade_id": "cell-bf4d39493ab7",
     "locked": true,
     "schema_version": 3
    }
   },
   "source": [
    "Now, we want to test the forward pass of the implemented neural network with a dummy input tensor.\n",
    "\n",
    "Generally, inputs are processed batch-wise by pyTorch Modules. Thus, the network expects an 2-dimensional tensor as input: the first dimensions corresponds to the batch size, and the second dimension contains the features for a single sample.\n",
    "\n",
    "Instantiate a tensor containing two samples (batch size 2) with all feature values equal to ``1``.\n",
    "Pass the tensor with the two samples through your neural network.\n",
    "\n",
    "What shape has the output tensor? How can the dimensions and entries be interpreted?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "15",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T08:52:08.201057Z",
     "start_time": "2024-11-06T08:52:08.188535Z"
    },
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-b3e4afcd8dfa",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3784, 0.3327, 0.2889],\n",
       "        [0.3784, 0.3327, 0.2889]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neural_net_instance = MyNeuralNetwork()\n",
    "### YOUR SOLUTION HERE\n",
    "input = torch.ones(2,5)\n",
    "neural_net_instance(input)\n",
    "### END OF SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {
    "nbgrader": {
     "grade_id": "cell-a78524fe701e",
     "locked": true,
     "schema_version": 3
    }
   },
   "source": [
    "### Interpreting Output Shapes\n",
    "\n",
    "When passing a batch of input samples through the neural network, the output tensor has the shape `(batch_size, num_classes)`. \n",
    "\n",
    "- In this example, the input tensor has shape `(2, 5)`, meaning there are 2 samples, each with 5 features.\n",
    "- The output tensor will have shape `(2, 3)`, corresponding to 2 samples and 3 output values (one for each class).\n",
    "\n",
    "Each row in the output tensor contains the predicted class probabilities (after the softmax activation) for a single input sample. The highest value in each row indicates the model's predicted class for that sample.\n",
    "\n",
    "**Example:**\n",
    "If the output is:\n",
    "\n",
    "```python\n",
    "tensor([[0.1, 0.7, 0.2], [0.3, 0.3, 0.4]])\n",
    "```\n",
    "\n",
    "- The first sample is most likely class 1 (0-based indexing).\n",
    "- The second sample is most likely class 2.\n",
    "\n",
    "This shape convention is standard for classification tasks in PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {
    "nbgrader": {
     "grade_id": "cell-970a39e9fc43",
     "locked": true,
     "schema_version": 3
    }
   },
   "source": [
    "### Model Parameter Counting\n",
    "\n",
    "The pyTorch class ``Module`` provides a number of very useful functionalities.\n",
    "\n",
    "For instance, if set up correctly, the trainable network parameters of the model can be accessed by the functions ``parameters()`` or ``named_parameters()``.\n",
    "In the following, the latter function is used to access all parameters names and values.\n",
    "Complete the code to compute and output the number of parameters for each named parameter, as well as the total number of parameters in your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "18",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T08:52:08.204920Z",
     "start_time": "2024-11-06T08:52:08.201057Z"
    },
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-72e0b7b467c6",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Named parameter: fc1.weight\n",
      "Number of parameters: 300\n",
      "Named parameter: fc1.bias\n",
      "Number of parameters: 60\n",
      "Named parameter: fc2.weight\n",
      "Number of parameters: 180\n",
      "Named parameter: fc2.bias\n",
      "Number of parameters: 3\n",
      "Total number of parameters: 543\n"
     ]
    }
   ],
   "source": [
    "sum_of_parameters = 0\n",
    "for par_name, par_values in neural_net_instance.named_parameters():\n",
    "    print(\"Named parameter: {}\".format(par_name))\n",
    "    ### YOUR SOLUTION HERE\n",
    "    print(\"Number of parameters: \" +  str(par_values.numel()))\n",
    "    sum_of_parameters += par_values.numel()\n",
    "    ### END OF SOLUTION\n",
    "print(\"Total number of parameters: {}\".format(sum_of_parameters))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {
    "collapsed": false,
    "nbgrader": {
     "grade_id": "cell-729fe3dd8309",
     "locked": true,
     "schema_version": 3
    }
   },
   "source": [
    "### Loss functions\n",
    "\n",
    "Like many important machine learning essentials, standard loss functions are already implemented in PyTorch.\n",
    "\n",
    "In the lecture, we have seen the frequently used Cross-Entropy-Loss. This loss is incorporated in the PyTorch-class ``torch.nn.CrossEntropyLoss``.\n",
    "The class can be initialized without arguments.\n",
    "To compute the loss, the class instance can be called taking the probabilities for each class as first, and the true class labels (data type ``long``) as second argument.\n",
    "\n",
    "Using the PyTorch-implementation, compute the cross entropy loss for a single sample in which class probabilites of ``[0.1, 0.8, 0.1]`` were predicted and the true class label is ``1``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "20",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T08:52:08.217359Z",
     "start_time": "2024-11-06T08:52:08.205997Z"
    },
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-1ab9f8fb82ea",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6897)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### YOUR SOLUTION HERE\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "pred = torch.tensor([0.1,0.8,0.1])\n",
    "true_value = torch.tensor([1]).long()\n",
    "loss(pred, true_value)\n",
    "### END OF SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {
    "nbgrader": {
     "grade_id": "cell-2f7b5d2b2fc2",
     "locked": true,
     "schema_version": 3
    }
   },
   "source": [
    "Which inputs are required to create an extremely low (high) loss value?\n",
    "\n",
    "Try to create and use inputs which achieve a loss value equal or close to ``0``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "22",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T08:52:08.222996Z",
     "start_time": "2024-11-06T08:52:08.218365Z"
    },
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-53bed1b07ec2",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### YOUR SOLUTION HERE\n",
    "pred = torch.tensor([[0., 50., 0.]])\n",
    "loss(pred, true_value)\n",
    "### END OF SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {
    "collapsed": false,
    "nbgrader": {
     "grade_id": "cell-e4ddb5ba5476",
     "locked": true,
     "schema_version": 3
    }
   },
   "source": [
    "### Optimizers\n",
    "\n",
    "Similarly, pyTorch also contains implementations of optimization algorithms like gradient descent.\n",
    "A broad variety of different [optimizers](https://pytorch.org/docs/stable/optim.html) are implemented.\n",
    "Nearly all of these optimizers are improved or more sophisticated versions of vanilla gradient descent, which we discussed in the lecture.\n",
    "We will use stochastic gradient descent, which is implemented in the class ``torch.optim.SGD``. \n",
    "\n",
    "Initialize a SGD optimizer, the constructor takes to optimizable parameters as argument (use the function ``parameter()`` of the above implemented model). Further, the named argument ``lr`` can be used to set the (initial) learning rate. Set that value to ``0.01``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "24",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T08:52:09.029744Z",
     "start_time": "2024-11-06T08:52:08.224003Z"
    },
    "collapsed": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-0a8d1208f4ba",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "### YOUR SOLUTION HERE\n",
    "sgd = torch.optim.SGD(neural_net_instance.parameters(), 0.01)\n",
    "### END OF SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {
    "nbgrader": {
     "grade_id": "cell-4f3c24b5e5a5",
     "locked": true,
     "schema_version": 3
    }
   },
   "source": [
    "Next, we will create an extremely small dummy dataset on which our previously defined model should be able to overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "26",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T08:52:09.035013Z",
     "start_time": "2024-11-06T08:52:09.030759Z"
    },
    "nbgrader": {
     "grade_id": "cell-e8f03d0fcd70",
     "locked": true,
     "schema_version": 3
    }
   },
   "outputs": [],
   "source": [
    "n_samples = 12\n",
    "dummy_training_data_x = torch.rand([n_samples, 5])\n",
    "dummy_training_data_y = torch.Tensor([i%3 for i in range(n_samples)]).long()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {
    "nbgrader": {
     "grade_id": "cell-895565d94374",
     "locked": true,
     "schema_version": 3
    }
   },
   "source": [
    "First, we want to implement a function ``compute_accuracy()`` which computes the model's accuracy.\n",
    "\n",
    "To compute the accuracy, complete the following steps:\n",
    "\n",
    "* Run data through model to get class probabilities\n",
    "* Find class prediction by picking highest probability for each sample\n",
    "* Compute relative number of samples in which class predictions coincides with annotated class\n",
    "* Return the computed accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "28",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T08:52:09.040958Z",
     "start_time": "2024-11-06T08:52:09.036029Z"
    },
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-eeb91e342b88",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(12) 0\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "invalid index of a 0-dim tensor. Use `tensor.item()` in Python or `tensor.item<T>()` in C++ to convert a 0-dim tensor to a number",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[64]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     17\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m pred\n\u001b[32m     19\u001b[39m     \u001b[38;5;66;03m### END OF SOLUTION\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mcompute_accuracy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mneural_net_instance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdummy_training_data_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdummy_training_data_y\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[64]\u001b[39m\u001b[32m, line 16\u001b[39m, in \u001b[36mcompute_accuracy\u001b[39m\u001b[34m(model, data_x, data_y)\u001b[39m\n\u001b[32m     14\u001b[39m         j += \u001b[32m1\u001b[39m\n\u001b[32m     15\u001b[39m     \u001b[38;5;28mprint\u001b[39m(preds, pred)\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m     \u001b[43mpreds\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m = pred\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m pred\n",
      "\u001b[31mIndexError\u001b[39m: invalid index of a 0-dim tensor. Use `tensor.item()` in Python or `tensor.item<T>()` in C++ to convert a 0-dim tensor to a number"
     ]
    }
   ],
   "source": [
    "def compute_accuracy(model, data_x, data_y):\n",
    "    ### YOUR SOLUTION HERE\n",
    "    output = model(data_x)\n",
    "    preds = torch.tensor(12)\n",
    "    i = 0\n",
    "    for sample in output:\n",
    "        pred = 0\n",
    "        best_pred = 0\n",
    "        j = 0\n",
    "        for prob in sample:\n",
    "            if prob > best_pred:\n",
    "                best_pred = prob\n",
    "                pred = j\n",
    "            j += 1\n",
    "        print(preds, pred)\n",
    "        preds[i] = pred\n",
    "    return pred\n",
    "\n",
    "    ### END OF SOLUTION\n",
    "print(compute_accuracy(neural_net_instance, dummy_training_data_x, dummy_training_data_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {
    "nbgrader": {
     "grade_id": "cell-539baaeb1c1a",
     "locked": true,
     "schema_version": 3
    }
   },
   "source": [
    "Before we start to train the model, i.e. to optimize the model parameters, we want to compute the model's accuracy in an untrained state (random weights).\n",
    "\n",
    "What accuracy value would you expect?\n",
    "\n",
    "Use the function ``compute_accuracy`` to compute the model accuracy for the dummy training data and print it to the console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T08:52:09.051999Z",
     "start_time": "2024-11-06T08:52:09.041964Z"
    },
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-245c0549e774",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "### YOUR SOLUTION HERE\n",
    "# expected accuracy: 0.33 equivalent to guessing one of three possible classes.\n",
    "### END OF SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {
    "nbgrader": {
     "grade_id": "cell-dd7208d9d807",
     "locked": true,
     "schema_version": 3
    }
   },
   "source": [
    "To perform a single training step in pyTorch, the following steps need to be performed:\n",
    "* Reset all gradients by calling the optimizer's method ``zero_grad()``\n",
    "* Pass training samples through model (call ``Module`` instance)\n",
    "* Loss computation\n",
    "* Compute backward pass using the optimizer's method ``backward()``\n",
    "* Update model parameters using the optimizer's method ``step()``\n",
    "\n",
    "The following function ``training_step`` performs all of the above to perform one gradient descent step and returns the computed loss value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T08:52:09.056860Z",
     "start_time": "2024-11-06T08:52:09.053080Z"
    },
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-b4385aa4d0c3",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def training_step(model: torch.nn.Module, loss: torch.nn.CrossEntropyLoss, optimizer: torch.optim.Optimizer, data_x: torch.Tensor, data_y: torch.Tensor):\n",
    "    optimizer.zero_grad()\n",
    "    prediction = model(data_x)\n",
    "    opt_target = loss(prediction, data_y)\n",
    "    opt_target.backward()\n",
    "    optimizer.step()\n",
    "    return opt_target.detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {
    "nbgrader": {
     "grade_id": "cell-2075c3cc82a5",
     "locked": true,
     "schema_version": 3
    }
   },
   "source": [
    "In the following, use all of the above variables and instances (model, loss, optimizer, dummy data, training function) to train your neural network on the dummy data.\n",
    "\n",
    "In each training epoch, iterate over the shuffled training samples. \n",
    "For each training sample, perform one stochastic gradient descent step.\n",
    "Sum of the losses for each sample to compute the epoch's overall loss and write it to ``epoch_losses``.\n",
    "When these steps are performed for all data samples, compute the model's accuracy after the training epoch.\n",
    "\n",
    "If the accuracy reaches 1.0 before all epochs are performed, training may be stopped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T08:52:11.980712Z",
     "start_time": "2024-11-06T08:52:09.057865Z"
    },
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-2419c7f7a6c4",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "indizes = np.arange(dummy_training_data_x.shape[0])\n",
    "epoch_losses = []\n",
    "\n",
    "for epoch in range(1000):\n",
    "    np.random.shuffle(indizes)\n",
    "    ### YOUR SOLUTION HERE\n",
    "    ### END OF SOLUTION\n",
    "    if acc == 1.0:\n",
    "        break\n",
    "\n",
    "print(\"Finished training after {} epochs. Model accuracy: {}\".format(epoch+1, acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {
    "nbgrader": {
     "grade_id": "cell-5bdbcf308722",
     "locked": true,
     "schema_version": 3
    }
   },
   "source": [
    "Use the following lines of code to plot your training progress (the evolution of the loss during training)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T08:52:12.664986Z",
     "start_time": "2024-11-06T08:52:11.981719Z"
    },
    "nbgrader": {
     "grade_id": "cell-37cf86329a14",
     "locked": true,
     "schema_version": 3
    }
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(epoch_losses)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
