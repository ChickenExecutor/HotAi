{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Classification with Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T12:45:05.349958Z",
     "start_time": "2025-11-17T12:45:05.278941Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T12:45:09.082614Z",
     "start_time": "2025-11-17T12:45:09.078786Z"
    }
   },
   "outputs": [],
   "source": [
    "# Sample corpus \n",
    "\n",
    "corpus_fox = [\n",
    "    \"the quick brown fox jumps over the lazy dog\",\n",
    "    \"the quick brown fox is very quick\",\n",
    "    \"the quick brown fox jumps\",\n",
    "    \"the lazy dog jumps over the quick fox\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "# Exercise: Bag-of-Words model from Scratch\n",
    "\n",
    "1) Define function to get the vocabulary of a corpus as a **sorted** list of unique words\n",
    "2) Define function to get the bag-of-words representation for the corpus as a 2D numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T12:45:10.366158Z",
     "start_time": "2025-11-17T12:45:10.362669Z"
    }
   },
   "outputs": [],
   "source": [
    "# define a function to get the unique list of words as corpus\n",
    "# take care that it is sorted\n",
    "def get_vocabulary(corpus):\n",
    "    \n",
    "    # start solution\n",
    "    vocab_set = set([word for doc in corpus for word in doc.split()])\n",
    "    vocab_list  = list(vocab_set)\n",
    "    vocab_list.sort()\n",
    "    # end solution\n",
    "    \n",
    "    return vocab_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T12:45:11.780054Z",
     "start_time": "2025-11-17T12:45:11.776593Z"
    }
   },
   "outputs": [],
   "source": [
    "vocab = get_vocabulary(corpus_fox)\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T12:45:12.636120Z",
     "start_time": "2025-11-17T12:45:12.631777Z"
    }
   },
   "outputs": [],
   "source": [
    "# test if your implementation is correct\n",
    "v_test = get_vocabulary(corpus_fox)\n",
    "assert len(v_test) == 10\n",
    "assert ['brown', 'dog', 'fox', 'is', 'jumps', 'lazy', 'over', 'quick', 'the', 'very'] == v_test\n",
    "\n",
    "print(\"=========== Tests passed =============\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T12:45:15.678523Z",
     "start_time": "2025-11-17T12:45:15.674464Z"
    }
   },
   "outputs": [],
   "source": [
    "# define a function to get the bag of words or term frequency matrix\n",
    "# it should return a 2D numpy array\n",
    "# each row represents a document and each column a word from the vocabulary\n",
    "\n",
    "def calculate_term_frequency_matrix(doc):\n",
    "    vocabulary = get_vocabulary(doc)\n",
    "    word_to_id = {word: i for i, word in enumerate(vocabulary)} # dict mapping from word to index\n",
    "    tf_matrix = np.zeros((len(doc), len(vocabulary))) \n",
    "    \n",
    "   # start solution\n",
    "    for i, d in enumerate(doc):\n",
    "        for word in d.split():\n",
    "            tf_matrix[i, word_to_id[word]] += 1\n",
    "    # end solution\n",
    "    \n",
    "    return tf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T12:45:17.096026Z",
     "start_time": "2025-11-17T12:45:17.088140Z"
    }
   },
   "outputs": [],
   "source": [
    "tf_matrix = calculate_term_frequency_matrix(corpus_fox)\n",
    "tf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T12:45:20.013273Z",
     "start_time": "2025-11-17T12:45:20.009071Z"
    }
   },
   "outputs": [],
   "source": [
    "# test if your implementation is correct\n",
    "test_matrix = np.array([[1., 1., 1., 0., 1., 1., 1., 1., 2., 0.],\n",
    "       [1., 0., 1., 1., 0., 0., 0., 2., 1., 1.],\n",
    "       [1., 0., 1., 0., 1., 0., 0., 1., 1., 0.],\n",
    "       [0., 1., 1., 0., 1., 1., 1., 1., 2., 0.]])\n",
    "\n",
    "assert np.array_equal(tf_matrix, test_matrix), \"The term frequency matrix is not correct\"\n",
    "print(\"=========== Tests passed =============\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "# Bag of Words model with scikit-learn\n",
    "\n",
    "This source code simply shows how to get the matrices using sklearn.  There are more libraries out there for statistical NLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T12:45:24.342003Z",
     "start_time": "2025-11-17T12:45:23.603174Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T12:45:24.918633Z",
     "start_time": "2025-11-17T12:45:24.911122Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create an instance of CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit the vectorizer to the corpus and transform the corpus into a Bag of Words matrix\n",
    "bow_matrix = vectorizer.fit_transform(corpus_fox)\n",
    "\n",
    "# Get the list of unique words (vocabulary)\n",
    "# it returns a sparse matrix!\n",
    "vocab = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Convert the Bag of Words matrix to a dense numpy array for easier manipulation\n",
    "bow_matrix_dense = bow_matrix.toarray()\n",
    "\n",
    "# Print the results\n",
    "print(\"Vocabulary:\")\n",
    "print(vocab)\n",
    "print(\"\\nSize of vocabulary:\", len(vocab))\n",
    "print(\"\\nBag of Words matrix (dense):\")\n",
    "print(bow_matrix_dense)\n",
    "print(\"\\nBag of Words matrix (sparse):\")\n",
    "print(bow_matrix)\n",
    "\n",
    "\n",
    "assert len(vocab) == 10, \"The length of the vocabulary is not correct\"\n",
    "assert np.array_equal(bow_matrix_dense, tf_matrix), \"The Bag of Words matrix is not correct\"\n",
    "\n",
    "print(\"=========== Tests passed =============\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "# N-grams with scikit-learn \n",
    "\n",
    "Almost the same steps as before, but we can specify the n-gram range when creating the CountVectorizer instance!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T12:45:38.896213Z",
     "start_time": "2025-11-17T12:45:38.890214Z"
    }
   },
   "outputs": [],
   "source": [
    "N = (3, 3)\n",
    "vectorizer = CountVectorizer(ngram_range=N)\n",
    "\n",
    "bow_matrix = vectorizer.fit_transform(corpus_fox)\n",
    "vocab = vectorizer.get_feature_names_out()\n",
    "bow_matrix_dense = bow_matrix.toarray()\n",
    "\n",
    "# Print the results\n",
    "print(\"Vocabulary:\")\n",
    "print(vocab)\n",
    "print(\"\\nSize of vocabulary:\", len(vocab))\n",
    "print(\"\\nBag of Words matrix (dense):\")\n",
    "print(bow_matrix_dense)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "# Classification with Bag of Words\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T12:45:40.762305Z",
     "start_time": "2025-11-17T12:45:40.674828Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Step 1: Training data\n",
    "corpus = [\n",
    "    'I love this product',\n",
    "    'This is a great product',\n",
    "    'I hate this product',\n",
    "    'This product is not good',\n",
    "    'I am very happy with this product',\n",
    "    'I am not satisfied with this product'\n",
    "]\n",
    "\n",
    "# Labels (1 for positive, 0 for negative)\n",
    "labels = np.array([1, 1, 0, 0, 1, 0])\n",
    "\n",
    "# Step 2: Use CountVectorizer to convert the corpus into a matrix of token counts\n",
    "# Create an instance of CountVectorizer with n-gram range (1, 2)\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 2))\n",
    "\n",
    "# Fit the vectorizer to the corpus and transform the corpus into n-gram count vectors\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Step 3: Train a Logistic Regression classifier\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(X, labels)\n",
    "\n",
    "# Evaluate the classifier on an unseen test set\n",
    "new_reviews = [\n",
    "    'I love this product, it is amazing!',\n",
    "    'I hate this product, it is terrible!',\n",
    "    'It is rather bad'\n",
    "]\n",
    "\n",
    "new_labels = np.array([1, 0,0])\n",
    "\n",
    "\n",
    "# start solution\n",
    "# Convert the new reviews into n-gram count vectors\n",
    "X_new = vectorizer.transform(new_reviews)\n",
    "print(vectorizer.vocabulary_)\n",
    "\n",
    "# Predict the labels for the test set\n",
    "y_pred = classifier.predict(X_new)\n",
    "\n",
    "print(y_pred)\n",
    "print(new_labels)\n",
    "# end solution\n",
    "\n",
    "# Evaluate the classifier\n",
    "accuracy = accuracy_score(new_labels, y_pred)\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "# accuracy should be 66.67%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "# Think about: Why is the last review misclassified?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
