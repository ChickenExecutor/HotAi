{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Language Model Metrics: Perplexity - Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Definition of Perplexity\n",
    "\n",
    "The commonly used metric to evaluate language models is called *Perplexity*.\n",
    "\n",
    "Assume data samples, i.e. sentences $x^{(i)}, i = 1, ..., N$ with every sentence consisting of a sequence of tokens (words) $x^{(i)} = x^{(i)}_1 x^{(i)}_2 ... x^{(i)}_{k_i}$, are given.\n",
    "The perplexity of a model on the given data is defined as \n",
    "\n",
    "$ PPL(\\theta) = \\exp \\left( - \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{k=1}^{k_i} \\log {P_{\\theta}(x^{(i)}_{k}|x^{(i)}_1...x^{(i)}_{k-1})} \\right) $\n",
    "\n",
    "where $\\theta$ are the model's parameters and $P_{\\theta}(x^{(i)}_{k}|x^{(i)}_1...x^{(i)}_{k-1})$ is the probability that the model outputs $x^{(i)}_{k}$ given the previous sequence of tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The following implementation of the Softmax function will be required to convert the model scores to probabilities.\n",
    "\n",
    "It is already known from a previous tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T13:30:21.415647Z",
     "start_time": "2025-11-17T13:30:21.338103Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def softmax(values):\n",
    "    exp_values = np.exp(values)\n",
    "    exp_values_sum = np.sum(exp_values)\n",
    "    return exp_values/exp_values_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Exercise: Implementation of Perplexity\n",
    "\n",
    "Implement the computation of the perplexity function as defined above.\n",
    "\n",
    "The function parameters are the scores output by the model (`token_scores`) of dimension (`n_samples`, `n_classes`), and the true class indices (`true_token_index`) which is an array of dimension (`n_samples`).\n",
    "\n",
    "In case the model scores do not resemble a probability distribution, the softmax function is applied to the scores for each prediction first. \n",
    "The last function parameter `apply_softmax` indicates whether a the softmax function should be applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T13:30:21.420416Z",
     "start_time": "2025-11-17T13:30:21.417020Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def perplexity(token_scores, true_token_index, apply_softmax=True):\n",
    "    log_prob_sum = 0\n",
    "    if apply_softmax:\n",
    "        token_probabilities = [softmax(scores) for scores in token_scores]\n",
    "    else:\n",
    "        token_probabilities = token_scores\n",
    "    \n",
    "    # start solution\n",
    "    for probs, true_index in zip(token_probabilities, true_token_index):\n",
    "        log_prob_sum += np.log(probs[true_index])\n",
    "    perplexity = np.exp(-log_prob_sum / len(true_token_index))\n",
    "    # end solution\n",
    "    \n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T13:30:21.428583Z",
     "start_time": "2025-11-17T13:30:21.421421Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### test implementation\n",
    "assert np.isclose(perplexity([[1e8, -1e8]], [0], apply_softmax=False), 0)\n",
    "assert np.isclose(perplexity([[1,0,0,0]], [0], apply_softmax=False), 1)\n",
    "assert np.isclose(perplexity([[0.5, 0.5]], [0], apply_softmax=False), 2)\n",
    "assert np.isclose(perplexity([[1, 2, 7], [2, -1, 0], [0, 1, 0], [1, 0.2, 0.2]], [2, 0, 1, 0], apply_softmax=True), 1.409032255704535)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Exercise: Perplexity function using Cross Entropy Loss\n",
    "\n",
    "You might have noticed that the perplexity function has a high similarity to the cross entropy loss which we have already seen in previous lectures and tutorials.\n",
    "Remember:\n",
    "\n",
    "$ \\text{CrossEntropyLoss} = - \\sum_{i=1}^{N} \\sum_{k=1}^{k_i} \\log {P_{\\theta}(x^{(i)}_{k}|x^{(i)}_1...x^{(i)}_{k-1})} $\n",
    "\n",
    "The cross entropy Loss is already implemented in PyTorch's class `torch.nn.CrossEntropyLoss` (compare previous tutorial).\n",
    "If the cross entropy loss is initialized without any parameters, the returned results will already be normed by the number of samples in the data (this could be avoided by setting the named parameter `reduction='sum'` or `reduction='none'` but is not necessary in this case). \n",
    "\n",
    "Use this existing implementation to compute the perplexity score based on the cross entropy loss.\n",
    "\n",
    "The parameters of the function `perplexity_ce_based` are identical to those of the previously implemented function `perplexity`.\n",
    "The first step in the implementation is to convert the given arrays to tensors, so they can be input to PyTorch's cross entropy computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T13:30:22.471295Z",
     "start_time": "2025-11-17T13:30:21.429592Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def perplexity_ce_based(token_scores, true_token_index):\n",
    "    token_scores_tensor = torch.tensor(token_scores)\n",
    "    true_token_index_tensor = torch.tensor(true_token_index).long()\n",
    "    # start solution\n",
    "    cross_entropy_loss = torch.nn.CrossEntropyLoss()\n",
    "    ce_loss = cross_entropy_loss(token_scores_tensor, true_token_index_tensor)\n",
    "    perplexity = np.exp(ce_loss.numpy())\n",
    "    # end solution\n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The following code cell initializes randomized numpy arrays which can be used to test your function implementations. \n",
    "\n",
    "If implemented correctly, the difference between the functions' return values should be extremely close to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T13:32:28.110353Z",
     "start_time": "2025-11-17T13:32:28.103760Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### randomized test case\n",
    "scores = np.double(np.random.random((12, 8)))\n",
    "true_classes = np.random.randint(0, 8, 12)\n",
    "\n",
    "ppl_1 = perplexity(scores, true_classes, apply_softmax=True)\n",
    "ppl_2 = perplexity_ce_based(scores, true_classes)\n",
    "\n",
    "ppl_diff = np.abs(ppl_1 - ppl_2)\n",
    "print(ppl_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Application of Perplexity\n",
    "\n",
    "Next, we will apply the computation of a perplexity score to the n_gram model from the previous exercise. \n",
    "\n",
    "The following code cell once again defines a small sample corpus and computes the corresponding ngram-frequencies for n=3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T13:32:36.770518Z",
     "start_time": "2025-11-17T13:32:36.765679Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "n = 3\n",
    "\n",
    "corpus = [\n",
    "    \"sos the fox is brown and quick eos\",\n",
    "    \"sos the dog is brown and lazy eos\",\n",
    "    \"sos the dog is very lazy eos\",\n",
    "    \"sos the fox is very quick eos\"\n",
    "]\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(n, n))  # Generate n-grams of size n\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "n_grams = vectorizer.get_feature_names_out()\n",
    "\n",
    "n_gram_freq = {}\n",
    "for ngram, count in zip(n_grams, X.toarray().sum(axis=0)):\n",
    "    n_gram_freq[ngram] = count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We use another CountVectorizer to get a list of all single tokens in the corpus vocabulary.\n",
    "This will be needed to compute the probabilities for each token based on the n-gram frequencies subsequently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T13:32:40.840422Z",
     "start_time": "2025-11-17T13:32:40.836172Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorizer_single_tokens = CountVectorizer(ngram_range=(1, 1))\n",
    "vectorizer_single_tokens.fit_transform(corpus)\n",
    "tokens = vectorizer_single_tokens.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We will use a single test sentence to compute the n-gram model's perplexity score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T13:32:41.795184Z",
     "start_time": "2025-11-17T13:32:41.790690Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "eval_text = [\"sos the fox is very quick eos\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Next, we will implement a function which computes the probabilities for each word in the vocabulary to be the next token (even if the corresponding n-gram does not occur in the corpus).\n",
    "These probabilities are required to compute a model perplexity score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T13:32:43.443584Z",
     "start_time": "2025-11-17T13:32:43.439004Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_next_token_probabilities(ngram_prefix, n_gram_freq, tokens):\n",
    "    candidates = {ngram: count for ngram, count in n_gram_freq.items() if ngram.startswith(ngram_prefix)}\n",
    "    freq_sum = sum(candidates.values())\n",
    "    # probs will be the list containing the probabilities for each token in tokens to be predicted by the n-gram language model \n",
    "    probs = []\n",
    "    # start solution\n",
    "    # for each token in tokens, compute the corresponding probability and append it to the list 'probs'\n",
    "    for token in tokens:\n",
    "        ngram = ngram_prefix + \" \" + token\n",
    "        if ngram in candidates:\n",
    "            prob = candidates[ngram] / freq_sum\n",
    "        else:\n",
    "            prob = 0\n",
    "        probs.append(prob)\n",
    "    # end solution\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Now, we will iterate over all n-grams in the evaluation text and compute the probabilties for each token to be output as final token of the n-gram.\n",
    "These probabilities are assembled in the array `eval_ngram_probabilities`.\n",
    "At the same time, the corresponding indices of the true next token are stored in the array `eval_true_tokens`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T13:32:48.931725Z",
     "start_time": "2025-11-17T13:32:48.926719Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "eval_ngram_probabilities = []\n",
    "eval_true_tokens = []\n",
    "\n",
    "# start solution\n",
    "for text in eval_text:\n",
    "    tokenized_text = text.split(\" \")\n",
    "    for i in range(len(tokenized_text) - n + 1):\n",
    "        ngram = \" \".join(tokenized_text[i:i+n-1])\n",
    "        next_token_probs = get_next_token_probabilities(ngram, n_gram_freq, tokens)\n",
    "        next_token_index = list(tokens).index(tokenized_text[i+n-1])\n",
    "        eval_ngram_probabilities.append(next_token_probs)\n",
    "        eval_true_tokens.append(next_token_index)\n",
    "# end solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Based on the previously computed token probabilities, we can compute a perplexity score.\n",
    "\n",
    "Note, that the softmax function should not be applied within the perplexity score computation as our model already outputs a probability distribution.\n",
    "(Multiple subsequent applications of the softmax function leads to levelling of the different scores.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T13:32:53.814776Z",
     "start_time": "2025-11-17T13:32:53.810652Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ppl = perplexity(eval_ngram_probabilities, eval_true_tokens, apply_softmax=False)\n",
    "print(ppl)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
