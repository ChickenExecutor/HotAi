{"metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}}, "nbformat": 4, "nbformat_minor": 5, "cells": [{"cell_type": "markdown", "id": "0", "metadata": {}, "source": "# Probabilistic Language Models\n\nN-gram for next Word Prediction: This is a very simple implementation using the frequency of n-grams to determine the next word.\n\n* Step 1: Generate n-grams using CountVectorizer\n* Step 2: Create a dictionary of n-gram frequencies\n* Step 3: Predict the next word based on the highest frequency n-gram"}, {"cell_type": "code", "execution_count": null, "id": "1", "metadata": {"ExecuteTime": {"end_time": "2024-11-13T12:59:57.872409Z", "start_time": "2024-11-13T12:59:56.744904Z"}}, "outputs": [], "source": "from sklearn.feature_extraction.text import CountVectorizer\nfrom collections import defaultdict"}, {"cell_type": "code", "execution_count": null, "id": "2", "metadata": {"ExecuteTime": {"end_time": "2024-11-13T12:59:57.879484Z", "start_time": "2024-11-13T12:59:57.874499Z"}}, "outputs": [], "source": "# Sample corpus\n\n# sos: start of sentence\n# eos: end of sentence\ncorpus = [\n    \"sos the quick brown fox jumps over the lazy dog eos\",\n    \"sos the quick brown fox is very quick eos\",\n    \"sos the quick brown fox jumps eos\",\n    \"sos the lazy dog jumps over the quick fox eos\"\n]"}, {"cell_type": "code", "execution_count": null, "id": "3", "metadata": {"ExecuteTime": {"end_time": "2024-11-13T12:59:57.905257Z", "start_time": "2024-11-13T12:59:57.882555Z"}}, "outputs": [], "source": "# Configurable n-gram size\nn = 3  # Adjust n here for different n-gram sizes (e.g., 2 for bigrams, 3 for trigrams)\n\n# Step 1: Generate n-grams using CountVectorizer\nvectorizer = CountVectorizer(ngram_range=(n, n))  # Generate n-grams of size n\nX = vectorizer.fit_transform(corpus)\nn_grams = vectorizer.get_feature_names_out()\n\nprint(\"*\" * 10)\nprint(n_grams)\nprint(X.toarray())\n\n# Step 2: Create a dictionary of n-gram frequencies\nn_gram_freq = defaultdict(int)\nfor ngram, count in zip(n_grams, X.toarray().sum(axis=0)):\n    n_gram_freq[ngram] = count\n\nprint(\"*\" * 10)\nprint(n_gram_freq)    \n    \n# Step 3: Predict the next word based on the highest frequency n-gram\ndef predict_next_word(prefix, n_gram_freq, n):\n    # Convert prefix to lowercase and strip any extra whitespace\n    prefix = prefix.lower().strip()\n    prefix_words = prefix.split()\n    \n    ### YOUR SOLUTION HERE\n    # Ensure the prefix has exactly n-1 words\n    # Find all n-grams that start with the given prefix\n    # print candidates for debugging\n    # If no candidates are found, return None\n    # Find the most frequent n-gram and extract the next word\n    ### END OF SOLUTION\n    \n    return next_word\n\n# Example prediction\nprefix = \" the quick \"  # For trigrams (n=3), the prefix should be two words\nnext_word = predict_next_word(prefix, n_gram_freq, n)\nprint(f\"Predicted next word after '{prefix}': {next_word}\")"}, {"cell_type": "code", "execution_count": null, "id": "4", "metadata": {"ExecuteTime": {"end_time": "2024-11-13T12:59:57.914293Z", "start_time": "2024-11-13T12:59:57.906547Z"}}, "outputs": [], "source": "# Example prediction for a longer sequence\nc = 0\n\nprefix = \"the quick\"\nwhile prefix.split()[-1] != \"eos\" and c < 50:\n    c += 1\n    print(f\"Round '{c}'\")\n    next_word = predict_next_word(prefix, n_gram_freq, n)\n    print(f\"Predicted next word after '{prefix}': {next_word}\")\n    if next_word == None:\n        break\n    new_prefix = prefix.split()[1:] #.append(next_word)\n    new_prefix.append(next_word)\n    prefix = \" \".join(new_prefix) + \" \"\n    print(prefix)"}]}