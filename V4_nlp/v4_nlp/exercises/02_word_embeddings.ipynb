{"metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}}, "nbformat": 4, "nbformat_minor": 5, "cells": [{"cell_type": "markdown", "id": "0", "metadata": {}, "source": "# Word Embeddings\n\nThis notebook contains no exercises.\nIt's just for playing with a static embedding! \nWe use the GloVe Vectors that are based on wikipedia with 200 dimensions."}, {"cell_type": "code", "id": "1", "metadata": {"collapsed": false}, "outputs": [], "source": "!pip install gensim"}, {"cell_type": "code", "execution_count": null, "id": "2", "metadata": {"ExecuteTime": {"end_time": "2024-11-13T12:52:38.937955Z", "start_time": "2024-11-13T12:52:37.229814Z"}}, "outputs": [], "source": "import numpy as np\nfrom sklearn.linear_model import LogisticRegression\n\nimport gensim.downloader as api"}, {"cell_type": "code", "execution_count": null, "id": "3", "metadata": {"ExecuteTime": {"end_time": "2024-11-13T12:53:43.873508Z", "start_time": "2024-11-13T12:52:40.362651Z"}}, "outputs": [], "source": "# Loading GloVe Embedding \ndef load_embedding_model():\n    \"\"\" Load GloVe Vectors\n        Return:\n            wv_from_bin: All 400000 embeddings, each lengh 200\n    \"\"\"\n\n    wv_from_bin = api.load(\"glove-wiki-gigaword-200\")\n    print(\"Loaded vocab size %i\" % len(list(wv_from_bin.index_to_key)))\n    return wv_from_bin\n\nwv_from_bin = load_embedding_model()"}, {"cell_type": "code", "execution_count": null, "id": "4", "metadata": {"ExecuteTime": {"end_time": "2024-11-13T12:54:33.538061Z", "start_time": "2024-11-13T12:54:33.515581Z"}}, "outputs": [], "source": "wv_from_bin.most_similar(\"like\")"}, {"cell_type": "code", "execution_count": null, "id": "5", "metadata": {"ExecuteTime": {"end_time": "2024-11-13T12:55:46.019569Z", "start_time": "2024-11-13T12:55:45.788147Z"}}, "outputs": [], "source": "import matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\n\ndef plot_word_embeddings(words, embeddings):\n    word_vectors = [embeddings[word] for word in words if word in embeddings]\n    pca = PCA(n_components=2)\n    word_vectors_2d = pca.fit_transform(word_vectors)\n    \n    plt.figure(figsize=(10, 10))\n    for i, word in enumerate(words):\n        if word in embeddings:\n            plt.scatter(word_vectors_2d[i, 0], word_vectors_2d[i, 1])\n            plt.text(word_vectors_2d[i, 0] + 0.01, word_vectors_2d[i, 1] + 0.01, word, fontsize=9)\n    plt.show()\n\n# Example usage\nwords = [\"king\", \"queen\", \"man\", \"woman\", \"apple\", \"orange\",]  # try more words such as baby, duchess, grape\nplot_word_embeddings(words, wv_from_bin)"}, {"cell_type": "markdown", "id": "6", "metadata": {}, "source": "# Using Word Embeddings for Classification"}, {"cell_type": "code", "execution_count": null, "id": "7", "metadata": {"ExecuteTime": {"end_time": "2024-11-13T12:56:45.634161Z", "start_time": "2024-11-13T12:56:45.622722Z"}}, "outputs": [], "source": "# Load pre-trained GloVe embeddings\ndef load_glove_embeddings():\n    return wv_from_bin\n\n# Convert text to average embedding\ndef text_to_embedding(text, embeddings, embedding_dim=200):\n    words = text.split()\n    word_vectors = [embeddings[word] for word in words if word in embeddings]\n    print(len(word_vectors)) \n    if len(word_vectors) > 0:\n        return np.mean(word_vectors, axis=0) # build the centroid\n    else:\n        return np.zeros(embedding_dim)\n\n# Example: Simple dataset\ntexts = [\"i love this movie\", \"this movie is terrible\", \"what a fantastic film\", \"i did not enjoy this movie\"]\nlabels = [1, 0, 1, 0]  # 1 = positive, 0 = negative\n\n# Load GloVe embeddings\nembeddings = load_glove_embeddings()  \n\n# Convert each text to its embedding\nX = np.array([text_to_embedding(text, embeddings) for text in texts])\n\n# Train a classifier (e.g., logistic regression)\nclf = LogisticRegression()\nclf.fit(X, labels)\n\n# Predict on a new example\nnew_text = (\"i love this movie\")\nnew_embedding = text_to_embedding(new_text, embeddings)\nprediction = clf.predict([new_embedding])\nprint(\"Predicted label:\", prediction)"}]}