{"metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}}, "nbformat": 4, "nbformat_minor": 5, "cells": [{"cell_type": "markdown", "id": "0", "metadata": {}, "source": "# Classification with Bag of Words"}, {"cell_type": "code", "execution_count": null, "id": "1", "metadata": {"ExecuteTime": {"end_time": "2025-11-17T12:45:05.349958Z", "start_time": "2025-11-17T12:45:05.278941Z"}}, "outputs": [], "source": "import numpy as np"}, {"cell_type": "code", "execution_count": null, "id": "2", "metadata": {"ExecuteTime": {"end_time": "2025-11-17T12:45:09.082614Z", "start_time": "2025-11-17T12:45:09.078786Z"}}, "outputs": [], "source": "# Sample corpus \n\ncorpus_fox = [\n    \"the quick brown fox jumps over the lazy dog\",\n    \"the quick brown fox is very quick\",\n    \"the quick brown fox jumps\",\n    \"the lazy dog jumps over the quick fox\"\n]"}, {"cell_type": "markdown", "id": "3", "metadata": {}, "source": "# Exercise: Bag-of-Words model from Scratch\n\n1) Define function to get the vocabulary of a corpus as a **sorted** list of unique words\n2) Define function to get the bag-of-words representation for the corpus as a 2D numpy array"}, {"cell_type": "code", "execution_count": null, "id": "4", "metadata": {"ExecuteTime": {"end_time": "2025-11-17T12:45:10.366158Z", "start_time": "2025-11-17T12:45:10.362669Z"}}, "outputs": [], "source": "# define a function to get the unique list of words as corpus\n# take care that it is sorted\ndef get_vocabulary(corpus):\n    \n    ### YOUR SOLUTION HERE\n    ### END OF SOLUTION\n    \n    return vocab_list"}, {"cell_type": "code", "execution_count": null, "id": "5", "metadata": {"ExecuteTime": {"end_time": "2025-11-17T12:45:11.780054Z", "start_time": "2025-11-17T12:45:11.776593Z"}}, "outputs": [], "source": "vocab = get_vocabulary(corpus_fox)\nprint(vocab)"}, {"cell_type": "code", "execution_count": null, "id": "6", "metadata": {"ExecuteTime": {"end_time": "2025-11-17T12:45:12.636120Z", "start_time": "2025-11-17T12:45:12.631777Z"}}, "outputs": [], "source": "# test if your implementation is correct\nv_test = get_vocabulary(corpus_fox)\nassert len(v_test) == 10\nassert ['brown', 'dog', 'fox', 'is', 'jumps', 'lazy', 'over', 'quick', 'the', 'very'] == v_test\n\nprint(\"=========== Tests passed =============\")"}, {"cell_type": "code", "execution_count": null, "id": "7", "metadata": {"ExecuteTime": {"end_time": "2025-11-17T12:45:15.678523Z", "start_time": "2025-11-17T12:45:15.674464Z"}}, "outputs": [], "source": "# define a function to get the bag of words or term frequency matrix\n# it should return a 2D numpy array\n# each row represents a document and each column a word from the vocabulary\n\ndef calculate_term_frequency_matrix(doc):\n    vocabulary = get_vocabulary(doc)\n    word_to_id = {word: i for i, word in enumerate(vocabulary)} # dict mapping from word to index\n    tf_matrix = np.zeros((len(doc), len(vocabulary))) \n    \n   ### YOUR SOLUTION HERE\n    ### END OF SOLUTION\n    \n    return tf_matrix"}, {"cell_type": "code", "execution_count": null, "id": "8", "metadata": {"ExecuteTime": {"end_time": "2025-11-17T12:45:17.096026Z", "start_time": "2025-11-17T12:45:17.088140Z"}}, "outputs": [], "source": "tf_matrix = calculate_term_frequency_matrix(corpus_fox)\ntf_matrix"}, {"cell_type": "code", "execution_count": null, "id": "9", "metadata": {"ExecuteTime": {"end_time": "2025-11-17T12:45:20.013273Z", "start_time": "2025-11-17T12:45:20.009071Z"}}, "outputs": [], "source": "# test if your implementation is correct\ntest_matrix = np.array([[1., 1., 1., 0., 1., 1., 1., 1., 2., 0.],\n       [1., 0., 1., 1., 0., 0., 0., 2., 1., 1.],\n       [1., 0., 1., 0., 1., 0., 0., 1., 1., 0.],\n       [0., 1., 1., 0., 1., 1., 1., 1., 2., 0.]])\n\nassert np.array_equal(tf_matrix, test_matrix), \"The term frequency matrix is not correct\"\nprint(\"=========== Tests passed =============\")"}, {"cell_type": "markdown", "id": "10", "metadata": {}, "source": "# Bag of Words model with scikit-learn\n\nThis source code simply shows how to get the matrices using sklearn.  There are more libraries out there for statistical NLP."}, {"cell_type": "code", "execution_count": null, "id": "11", "metadata": {"ExecuteTime": {"end_time": "2025-11-17T12:45:24.342003Z", "start_time": "2025-11-17T12:45:23.603174Z"}}, "outputs": [], "source": "from sklearn.feature_extraction.text import CountVectorizer"}, {"cell_type": "code", "execution_count": null, "id": "12", "metadata": {"ExecuteTime": {"end_time": "2025-11-17T12:45:24.918633Z", "start_time": "2025-11-17T12:45:24.911122Z"}}, "outputs": [], "source": "# Create an instance of CountVectorizer\nvectorizer = CountVectorizer()\n\n# Fit the vectorizer to the corpus and transform the corpus into a Bag of Words matrix\nbow_matrix = vectorizer.fit_transform(corpus_fox)\n\n# Get the list of unique words (vocabulary)\n# it returns a sparse matrix!\nvocab = vectorizer.get_feature_names_out()\n\n# Convert the Bag of Words matrix to a dense numpy array for easier manipulation\nbow_matrix_dense = bow_matrix.toarray()\n\n# Print the results\nprint(\"Vocabulary:\")\nprint(vocab)\nprint(\"\\nSize of vocabulary:\", len(vocab))\nprint(\"\\nBag of Words matrix (dense):\")\nprint(bow_matrix_dense)\nprint(\"\\nBag of Words matrix (sparse):\")\nprint(bow_matrix)\n\n\nassert len(vocab) == 10, \"The length of the vocabulary is not correct\"\nassert np.array_equal(bow_matrix_dense, tf_matrix), \"The Bag of Words matrix is not correct\"\n\nprint(\"=========== Tests passed =============\")"}, {"cell_type": "markdown", "id": "13", "metadata": {}, "source": "# N-grams with scikit-learn \n\nAlmost the same steps as before, but we can specify the n-gram range when creating the CountVectorizer instance!"}, {"cell_type": "code", "execution_count": null, "id": "14", "metadata": {"ExecuteTime": {"end_time": "2025-11-17T12:45:38.896213Z", "start_time": "2025-11-17T12:45:38.890214Z"}}, "outputs": [], "source": "N = (3, 3)\nvectorizer = CountVectorizer(ngram_range=N)\n\nbow_matrix = vectorizer.fit_transform(corpus_fox)\nvocab = vectorizer.get_feature_names_out()\nbow_matrix_dense = bow_matrix.toarray()\n\n# Print the results\nprint(\"Vocabulary:\")\nprint(vocab)\nprint(\"\\nSize of vocabulary:\", len(vocab))\nprint(\"\\nBag of Words matrix (dense):\")\nprint(bow_matrix_dense)"}, {"cell_type": "markdown", "id": "15", "metadata": {}, "source": "# Classification with Bag of Words\n\n"}, {"cell_type": "code", "execution_count": null, "id": "16", "metadata": {"ExecuteTime": {"end_time": "2025-11-17T12:45:40.762305Z", "start_time": "2025-11-17T12:45:40.674828Z"}}, "outputs": [], "source": "import numpy as np\nfrom sklearn.feature_extraction.text import CountVectorizer\n# from sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\n\n# Step 1: Training data\ncorpus = [\n    'I love this product',\n    'This is a great product',\n    'I hate this product',\n    'This product is not good',\n    'I am very happy with this product',\n    'I am not satisfied with this product'\n]\n\n# Labels (1 for positive, 0 for negative)\nlabels = np.array([1, 1, 0, 0, 1, 0])\n\n# Step 2: Use CountVectorizer to convert the corpus into a matrix of token counts\n# Create an instance of CountVectorizer with n-gram range (1, 2)\nvectorizer = CountVectorizer(ngram_range=(1, 2))\n\n# Fit the vectorizer to the corpus and transform the corpus into n-gram count vectors\nX = vectorizer.fit_transform(corpus)\n\n# Step 3: Train a Logistic Regression classifier\nclassifier = LogisticRegression()\nclassifier.fit(X, labels)\n\n# Evaluate the classifier on an unseen test set\nnew_reviews = [\n    'I love this product, it is amazing!',\n    'I hate this product, it is terrible!',\n    'It is rather bad'\n]\n\nnew_labels = np.array([1, 0,0])\n\n\n### YOUR SOLUTION HERE\n# Convert the new reviews into n-gram count vectors\n# Predict the labels for the test set\n### END OF SOLUTION\n\n# Evaluate the classifier\naccuracy = accuracy_score(new_labels, y_pred)\nprint(f'Accuracy: {accuracy * 100:.2f}%')\n# accuracy should be 66.67%"}, {"cell_type": "markdown", "id": "17", "metadata": {}, "source": "# Think about: Why is the last review misclassified?"}]}