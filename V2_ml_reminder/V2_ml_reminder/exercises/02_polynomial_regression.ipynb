{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ###  Polynomial Regression - Exercises\n",
    "\n",
    " ---\n",
    "\n",
    "In this exercise, we will perform polynomial regression as introduced in the lecture.\n",
    "\n",
    "Our toy problem will be to based on the sine function, which we will use to sample a dataset.\n",
    "We will try to fit a polynomial to approximate the values sampled from the sine function and investigate how the degree of the polynomial influences the result quality.\n",
    "\n",
    "We will also add regularization to the regression model and perform ridge regression on the same dataset.\n",
    "\n",
    "You are more than welcome to go beyond the concrete tasks in this notebook and, e.g., vary underlying function, data amount, or parameters.\n",
    "\n",
    "#### Setup\n",
    "Once again, we start by importing required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# setting seed for reproducibility\n",
    "np.random.seed(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the function ```true_function()``` which is the function we will try to estimate using linear regression in this exercise.\n",
    "The function takes an array of x-values and returns the corresponding sine-values at the positions $2 \\pi \\cdot x$. \n",
    "\n",
    "The function ```np.sin()``` applies the sine function pointwise and can be used to compute the sine of all array entries at once. \n",
    "The constant $\\pi$ is available as ``np.pi``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR SOLUTION HERE\n",
    "def true_function(x):\n",
    "    return np.sin(2*np.pi*x)\n",
    "### END OF SOLUTION\n",
    "\n",
    "# sanity check: function value for x=1 and x=0.5 should be (close to) 0.\n",
    "assert abs(true_function(np.array([1])))[0] < 1e-9\n",
    "assert abs(true_function(np.array([0.5])))[0] < 1e-9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will create sample data for training and testing of our polynomial regression model.\n",
    "\n",
    "Instantiate arrays ```x_train``` and ```x_test``` of dimensions ```(n_samples_train, 1)``` and ```(n_samples_test, 1)```, respectively. \n",
    "Once we have fitted our polynomial to the training data we will use the test data to evalute how well the predicted function generalizes to new data.\n",
    "\n",
    "The values for both arrays shall be sampled from a uniform distribution in the range of 0 to 1. Therefore, use ```numpy's``` function ```np.random.uniform()```.\n",
    "\n",
    "Using the previously implemented function ```true_function()```, the corresponding y-values ```y_train``` and ```y_test``` can be computed.\n",
    "Analogously to the previous exercise, add normally distributed noise of mean 0 and standard deviation 0.15 to the values of ```y_train``` and ```y_test``` (use ``np.random.normal()``)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of input sampels\n",
    "n_samples_train = 12\n",
    "n_samples_test = 10\n",
    "\n",
    "### YOUR SOLUTION HERE\n",
    "# sampling x-values in the range (0, 1)\n",
    "# sampling gaussian noise\n",
    "# adding noise to obtain the noisy y-values for training and test data\n",
    "### END OF SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Polynomial feature creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the function ```compute_polynomial_features()``` which is used for the creation of the polynomial features (in the 1-dimensional case). \n",
    "\n",
    "The function takes two arguments: an array of x-values of shape ``(n,1)`` and the degree ``k`` of the polynomial.\n",
    "It returns the corresponding polynomial features in a 2-dimensional array of shape ``(n,k+1)``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR SOLUTION HERE\n",
    "### END OF SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean Squared Error\n",
    "\n",
    "In order to evaluate performance of the regression, implement the function ```mse()``` to compute the mean squared error for the training and test data. \n",
    "For two input arrays ```y_pred``` and ```y_true``` of identical shape ``(n,1)`` the function returns the mean squared error (single scalar value) as defined in the lecture.\n",
    "\n",
    "We also create an instance of PyTorch's class ``MSELoss`` to later compare our results to the existing implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR SOLUTION HERE\n",
    "### END OF SOLUTION\n",
    "\n",
    "# pytorch implementation of mse\n",
    "loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Polynomial Regression\n",
    "\n",
    "We use the function ```compute_polynomial_features()``` to create matrices ```x_train_polynomial``` and ```x_test_polynomial``` containing all the features required for polynomial regression.\n",
    "\n",
    "In a next step, we fit a polynomial function to our test data by computing the polynomial parameters $ w_{\\text{pred}} $ using the closed form solution $ \\mathbf{w}_{\\text{pred}} = (\\mathbf{X}_{\\text{train}}^{\\top} \\mathbf{X}_{\\text{train}})^{-1} \\mathbf{X}_{\\text{train}}^{\\top} \\mathbf{y}_{\\text{train}}$.\n",
    "\n",
    "Given the predicted parameters we compute the y-values predicted by the fitted polynomial function for both our training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate polynomial features for diven polynomial degree using implemented method\n",
    "degree = 3\n",
    "x_train_polynomial = compute_polynomial_features(x_train, degree)\n",
    "x_test_polynomial = compute_polynomial_features(x_test, degree)\n",
    "\n",
    "### YOUR SOLUTION HERE\n",
    "### END OF SOLUTION\n",
    "\n",
    "# compute train and test predictions using computed weights \n",
    "y_pred_train = np.matmul(x_train_polynomial, w_pred)\n",
    "y_pred_test = np.matmul(x_test_polynomial, w_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will compute the mean squared error for test and training data using the previously implemented function ``mse()``.\n",
    "Store the results in two variables ``mse_train`` and ``mse_test``, respectively.\n",
    "\n",
    "We compare the results returned by ```mse()``` to the pytorch implementation of the MSE. \n",
    "Apart from rounding errors, they should both return identical values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR SOLUTION HERE\n",
    "# compute mse using implemented mse method\n",
    "### END OF SOLUTION\n",
    "\n",
    "# compute mse using pytorch implementation for comparison\n",
    "mse_train_torch = loss_fn(torch.tensor(y_pred_train), torch.tensor(y_train))\n",
    "mse_test_torch = loss_fn(torch.tensor(y_pred_test), torch.tensor(y_test))\n",
    "\n",
    "# check that custom mse implementation equals pytorch implementation\n",
    "assert np.isclose(mse_train, mse_train_torch)\n",
    "assert np.isclose(mse_test, mse_test_torch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print the resulting error values to the console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Polynomial of degree {degree}: \\n\")\n",
    "print(f\"MSE Training: {mse_train}\")\n",
    "print(f\"MSE Test: {mse_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Visualization\n",
    " \n",
    "The function ```visualize_results()``` can be used to visualize the estimated polynomial function alongside the training data and the true function, i.e. the sine function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_results(x_train, y_train, x_test, y_test, w_pred, degree):\n",
    "    fig, axes = plt.subplots(1, 1, figsize=(6, 6))\n",
    "    axes.scatter(x_train, y_train, label='Train Samples', color='seagreen')\n",
    "    axes.scatter(x_test, y_test, label='Test Samples', color='orangered')\n",
    "\n",
    "    x = np.linspace(0, 1, 1000).reshape(-1, 1)\n",
    "    x_feat = compute_polynomial_features(x, degree)\n",
    "    y_true = true_function(x)\n",
    "    y_pred = np.matmul(x_feat, w_pred)\n",
    "    axes.plot(x, y_true, label='True Function')\n",
    "    axes.plot(x, y_pred, label='Predicted Function', color='seagreen', linestyle='--')\n",
    "    \n",
    "    axes.set_ylim([-1.5, 1.5])\n",
    "    axes.set_xlabel(r'$x$', fontsize=14)\n",
    "    axes.set_ylabel(r'$y$', fontsize=14)\n",
    "    plt.legend(fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use ``visualize_results()`` to manually inspect the results.\n",
    "\n",
    "Try different polynomial degrees and investigate how the results vary.\n",
    "\n",
    "What can you say about the training samples and the amount thereof? Feel free to try again with different amounts of training (test) data.\n",
    " \n",
    "What happens if x-values outside the interval $[0, 1)$ are considered at test time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_results(x_train, y_train, x_test, y_test, w_pred, degree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will add regularization to the same regression problem.\n",
    "\n",
    "To do so, first compute the regularization term using ``lambda_reg`` as parameter $\\lambda$. You can make use of ```np.eye()``` to create an identity matrix for the computation of the regularization term.\n",
    "\n",
    "Then, implement the closed form solution including regularization term to compute the parameters ```w_pred_reg```.\n",
    "\n",
    "Compare the magnitudes of the predicted polynomial parameters ```w_pred_reg``` to the previous solution ```w_pred```. What can you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_reg = 1e-5\n",
    "\n",
    "### YOUR SOLUTION HERE\n",
    "# Compute regularization term\n",
    "# Compute polynomial parameters using closed form solution including regularization\n",
    "### END OF SOLUTION\n",
    "\n",
    "y_pred_train_ridge = np.matmul(x_train_polynomial, w_pred_reg)\n",
    "y_pred_test_ridge = np.matmul(x_test_polynomial, w_pred_reg)\n",
    "\n",
    "mse_train_ridge = mse(y_pred_train_ridge, y_train)\n",
    "mse_test_ridge = mse(y_pred_test_ridge, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, print computed MSE values.\n",
    "\n",
    "How does the regularization and its magnitude influence the error values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Polynomial of degree {degree}: \\n\")\n",
    "print(f\"MSE Training Ridge Regression: {mse_train_ridge}\")\n",
    "print(f\"MSE Test Ridge Regression: {mse_test_ridge}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The function ``visualize_results()`` can again be used to inspect the results in a 2-dimensional plot.\n",
    " \n",
    " Try different regularization values $\\lambda$ and investigate how the results change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_results(x_train, y_train, x_test, y_test, w_pred_reg, degree)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
