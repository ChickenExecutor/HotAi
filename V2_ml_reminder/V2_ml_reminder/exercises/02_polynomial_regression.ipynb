{"metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}}, "nbformat": 4, "nbformat_minor": 2, "cells": [{"cell_type": "markdown", "metadata": {}, "source": " ###  Polynomial Regression - Exercises\n\n ---\n\nIn this exercise, we will perform polynomial regression as introduced in the lecture.\n\nOur toy problem will be to based on the sine function, which we will use to sample a dataset.\nWe will try to fit a polynomial to approximate the values sampled from the sine function and investigate how the degree of the polynomial influences the result quality.\n\nWe will also add regularization to the regression model and perform ridge regression on the same dataset.\n\nYou are more than welcome to go beyond the concrete tasks in this notebook and, e.g., vary underlying function, data amount, or parameters.\n\n#### Setup\nOnce again, we start by importing required packages."}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# imports\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\n\n# setting seed for reproducibility\nnp.random.seed(2)"}, {"cell_type": "markdown", "metadata": {}, "source": "#### Input sample creation"}, {"cell_type": "markdown", "metadata": {}, "source": "Implement the function ```true_function()``` which is the function we will try to estimate using linear regression in this exercise.\nThe function takes an array of x-values and returns the corresponding sine-values at the positions $2 \\pi \\cdot x$. \n\nThe function ```np.sin()``` applies the sine function pointwise and can be used to compute the sine of all array entries at once. \nThe constant $\\pi$ is available as ``np.pi``."}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "### YOUR SOLUTION HERE\n### END OF SOLUTION\n\n# sanity check: function value for x=1 and x=0.5 should be (close to) 0.\nassert abs(true_function(np.array([1])))[0] < 1e-9\nassert abs(true_function(np.array([0.5])))[0] < 1e-9"}, {"cell_type": "markdown", "metadata": {}, "source": "Next, we will create sample data for training and testing of our polynomial regression model.\n\nInstantiate arrays ```x_train``` and ```x_test``` of dimensions ```(n_samples_train, 1)``` and ```(n_samples_test, 1)```, respectively. \nOnce we have fitted our polynomial to the training data we will use the test data to evalute how well the predicted function generalizes to new data.\n\nThe values for both arrays shall be sampled from a uniform distribution in the range of 0 to 1. Therefore, use ```numpy's``` function ```np.random.uniform()```.\n\nUsing the previously implemented function ```true_function()```, the corresponding y-values ```y_train``` and ```y_test``` can be computed.\nAnalogously to the previous exercise, add normally distributed noise of mean 0 and standard deviation 0.15 to the values of ```y_train``` and ```y_test``` (use ``np.random.normal()``)."}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# number of input sampels\nn_samples_train = 12\nn_samples_test = 10\n\n### YOUR SOLUTION HERE\n# sampling x-values in the range (0, 1)\n# sampling gaussian noise\n# adding noise to obtain the noisy y-values for training and test data\n### END OF SOLUTION"}, {"cell_type": "markdown", "metadata": {}, "source": "#### Polynomial feature creation"}, {"cell_type": "markdown", "metadata": {}, "source": "Implement the function ```compute_polynomial_features()``` which is used for the creation of the polynomial features (in the 1-dimensional case). \n\nThe function takes two arguments: an array of x-values of shape ``(n,1)`` and the degree ``k`` of the polynomial.\nIt returns the corresponding polynomial features in a 2-dimensional array of shape ``(n,k+1)``."}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "### YOUR SOLUTION HERE\n### END OF SOLUTION"}, {"cell_type": "markdown", "metadata": {}, "source": "#### Mean Squared Error\n\nIn order to evaluate performance of the regression, implement the function ```mse()``` to compute the mean squared error for the training and test data. \nFor two input arrays ```y_pred``` and ```y_true``` of identical shape ``(n,1)`` the function returns the mean squared error (single scalar value) as defined in the lecture.\n\nWe also create an instance of PyTorch's class ``MSELoss`` to later compare our results to the existing implementation."}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "### YOUR SOLUTION HERE\n### END OF SOLUTION\n\n# pytorch implementation of mse\nloss_fn = nn.MSELoss()"}, {"cell_type": "markdown", "metadata": {}, "source": "#### Polynomial Regression\n\nWe use the function ```compute_polynomial_features()``` to create matrices ```x_train_polynomial``` and ```x_test_polynomial``` containing all the features required for polynomial regression.\n\nIn a next step, we fit a polynomial function to our test data by computing the polynomial parameters $ w_{\\text{pred}} $ using the closed form solution $ \\mathbf{w}_{\\text{pred}} = (\\mathbf{X}_{\\text{train}}^{\\top} \\mathbf{X}_{\\text{train}})^{-1} \\mathbf{X}_{\\text{train}}^{\\top} \\mathbf{y}_{\\text{train}}$.\n\nGiven the predicted parameters we compute the y-values predicted by the fitted polynomial function for both our training and test data."}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# generate polynomial features for diven polynomial degree using implemented method\ndegree = 3\nx_train_polynomial = compute_polynomial_features(x_train, degree)\nx_test_polynomial = compute_polynomial_features(x_test, degree)\n\n### YOUR SOLUTION HERE\n### END OF SOLUTION\n\n# compute train and test predictions using computed weights \ny_pred_train = np.matmul(x_train_polynomial, w_pred)\ny_pred_test = np.matmul(x_test_polynomial, w_pred)"}, {"cell_type": "markdown", "metadata": {}, "source": "Next, we will compute the mean squared error for test and training data using the previously implemented function ``mse()``.\nStore the results in two variables ``mse_train`` and ``mse_test``, respectively.\n\nWe compare the results returned by ```mse()``` to the pytorch implementation of the MSE. \nApart from rounding errors, they should both return identical values."}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "### YOUR SOLUTION HERE\n# compute mse using implemented mse method\n### END OF SOLUTION\n\n# compute mse using pytorch implementation for comparison\nmse_train_torch = loss_fn(torch.tensor(y_pred_train), torch.tensor(y_train))\nmse_test_torch = loss_fn(torch.tensor(y_pred_test), torch.tensor(y_test))\n\n# check that custom mse implementation equals pytorch implementation\nassert np.isclose(mse_train, mse_train_torch)\nassert np.isclose(mse_test, mse_test_torch)"}, {"cell_type": "markdown", "metadata": {}, "source": "Let's print the resulting error values to the console."}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "print(f\"Polynomial of degree {degree}: \\n\")\nprint(f\"MSE Training: {mse_train}\")\nprint(f\"MSE Test: {mse_test}\")"}, {"cell_type": "markdown", "metadata": {}, "source": " #### Visualization\n \nThe function ```visualize_results()``` can be used to visualize the estimated polynomial function alongside the training data and the true function, i.e. the sine function."}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "def visualize_results(x_train, y_train, x_test, y_test, w_pred, degree):\n    fig, axes = plt.subplots(1, 1, figsize=(6, 6))\n    axes.scatter(x_train, y_train, label='Train Samples', color='seagreen')\n    axes.scatter(x_test, y_test, label='Test Samples', color='orangered')\n\n    x = np.linspace(0, 1, 1000).reshape(-1, 1)\n    x_feat = compute_polynomial_features(x, degree)\n    y_true = true_function(x)\n    y_pred = np.matmul(x_feat, w_pred)\n    axes.plot(x, y_true, label='True Function')\n    axes.plot(x, y_pred, label='Predicted Function', color='seagreen', linestyle='--')\n    \n    axes.set_ylim([-1.5, 1.5])\n    axes.set_xlabel(r'$x$', fontsize=14)\n    axes.set_ylabel(r'$y$', fontsize=14)\n    plt.legend(fontsize=14)"}, {"cell_type": "markdown", "metadata": {}, "source": "Use ``visualize_results()`` to manually inspect the results.\n\nTry different polynomial degrees and investigate how the results vary.\n\nWhat can you say about the training samples and the amount thereof? Feel free to try again with different amounts of training (test) data.\n \nWhat happens if x-values outside the interval $[0, 1)$ are considered at test time?"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "visualize_results(x_train, y_train, x_test, y_test, w_pred, degree)"}, {"cell_type": "markdown", "metadata": {}, "source": "### Ridge Regression"}, {"cell_type": "markdown", "metadata": {}, "source": "Next, we will add regularization to the same regression problem.\n\nTo do so, first compute the regularization term using ``lambda_reg`` as parameter $\\lambda$. You can make use of ```np.eye()``` to create an identity matrix for the computation of the regularization term.\n\nThen, implement the closed form solution including regularization term to compute the parameters ```w_pred_reg```.\n\nCompare the magnitudes of the predicted polynomial parameters ```w_pred_reg``` to the previous solution ```w_pred```. What can you observe?"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "lambda_reg = 1e-5\n\n### YOUR SOLUTION HERE\n# Compute regularization term\n# Compute polynomial parameters using closed form solution including regularization\n### END OF SOLUTION\n\ny_pred_train_ridge = np.matmul(x_train_polynomial, w_pred_reg)\ny_pred_test_ridge = np.matmul(x_test_polynomial, w_pred_reg)\n\nmse_train_ridge = mse(y_pred_train_ridge, y_train)\nmse_test_ridge = mse(y_pred_test_ridge, y_test)"}, {"cell_type": "markdown", "metadata": {}, "source": "Again, print computed MSE values.\n\nHow does the regularization and its magnitude influence the error values?"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "print(f\"Polynomial of degree {degree}: \\n\")\nprint(f\"MSE Training Ridge Regression: {mse_train_ridge}\")\nprint(f\"MSE Test Ridge Regression: {mse_test_ridge}\")"}, {"cell_type": "markdown", "metadata": {}, "source": " The function ``visualize_results()`` can again be used to inspect the results in a 2-dimensional plot.\n \n Try different regularization values $\\lambda$ and investigate how the results change."}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "visualize_results(x_train, y_train, x_test, y_test, w_pred_reg, degree)"}]}