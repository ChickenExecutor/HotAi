{"metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}}, "nbformat": 4, "nbformat_minor": 2, "cells": [{"cell_type": "markdown", "metadata": {}, "source": "# Regression with PyTorch - Exercises\n\nPyTorch is one of the most-used pthon library for deep learning, in a broad variety of applications.\n\nEven though it is not designed for simple regression problems, it can be used to solve such tasks. "}, {"cell_type": "markdown", "metadata": {}, "source": "### Setup\n\nThe following lines import the packages required in this exercise."}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "import numpy as np\nimport torch\nimport torch.nn as nn\nimport matplotlib.pyplot as plt\nfrom IPython import display\nimport time\n%matplotlib inline"}, {"cell_type": "markdown", "metadata": {}, "source": "### Regression using PyTorch"}, {"cell_type": "markdown", "metadata": {}, "source": "#### Data Creation\n\nWe explore linear regression in PyTorch using the same problem description as previously in exercise \"01_linear_regression\".\n\nYou are welcome to copy the code to create training data arrays ``X_train`` and ``y_train``.\n\n(Once again, use parameter values ``w0=2.0``, ``w1=1.5`` and add Gaussian noise of standard deviation 0.15 to the y-values.)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# number of input samples\nn_samples = 15\n\n### YOUR SOLUTION HERE\n# x_1 values in the range (0, 10)\n# constants x_0 of the same shape as x_1\n# input data X\n# true model parameters\n# output data y \n### END OF SOLUTION"}, {"cell_type": "markdown", "metadata": {}, "source": "Next, we convert the training data (x- and y-values) into a PyTorch Tensors, which is necessary for subsequent use with the PyTorch library."}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "training_data_x = torch.Tensor(x1_train) \ntraining_data_y = torch.Tensor(y_train)"}, {"cell_type": "markdown", "metadata": {}, "source": "In this exercise, the linear regression problem will not be solved using the closed form solution, but an iterative loss minimization approach.\n\nEven though we have not covered the theory yet (we will do so next week),\na simple artificial neuron (without non-linear activation function) is methodically identical to basic linear regression.\nA simple neural layer is implemented in PyTorch's class ``nn.Linear()``, and a single neuron model can be initialized by ``nn.Linear(1, 1)``.\n\nFirst, implement the class ```LinearRegressor``` which inherits from ```nn.Module```. In the ```__init__``` function add a linear layer using ```nn.Linear()```. This layer takes a single input value x and outputs the corresponding prediction y_pred. Also implement the ```forward(self, x)``` for the forward pass of the regressor. For details see the documentation at https://pytorch.org/docs/stable/generated/torch.nn.Module.html."}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "### YOUR SOLUTION HERE\n### END OF SOLUTION"}, {"cell_type": "markdown", "metadata": {}, "source": "Using the (parameter-free) constructor of our model class ``LinearRegressor``, the model can be initialized.\n\nIn this step, model parameters are initialized randomly."}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "regressor = LinearRegressor()"}, {"cell_type": "markdown", "metadata": {}, "source": "Next, we implement a method to plot the training progress during training using matplotlib and the Jupyter Matplotlib inline feature."}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "def update_viz(fig, axes, losses, training_data_x, training_data_y):\n    x_viz = torch.linspace(0, 10, 100).reshape(-1, 1)\n    y_viz = regressor(x_viz).detach().numpy()\n    axes[0].cla()\n    axes[1].cla()\n    axes[0].set_xlim([0, 10])\n    axes[0].set_ylim([0, 20])\n    axes[1].set_xlim([0, 100])\n    axes[1].set_ylim([0, 100])\n    axes[1].plot(losses, label='mean squared error')\n    axes[0].scatter(training_data_x.detach().numpy(), training_data_y.detach().numpy(), label='training samples')\n    axes[0].plot(x_viz, y_viz, color='red', label='current model')\n    axes[0].legend()\n    axes[1].legend()\n    display.clear_output(wait=True)\n    display.display(plt.gcf())\n    time.sleep(0.001)"}, {"cell_type": "markdown", "metadata": {}, "source": "The following code block performs the training with PyTorch.\n\nUsing already implemented loss functions and gradients, the computed loss for model prediction's and training data is iteratively minimized, thereby finding the optimal training parameters.\n\nDuring training, the training progress is illustrated below.\n\nYou can try different learning rates and also vary the training data to see how training process and convergence speed vary.\n\n*Attention:* Model parameters are stored in the LinearRegressor instance. If you change the setup, re-initialize the model to reset the training progress."}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\nn_episodes = 100\n\nloss_fn = nn.MSELoss()\n\nlearning_rate = 5e-4\noptimizer = torch.optim.SGD(regressor.parameters(), lr=learning_rate)\n\nlosses = []\nfor i in range(n_episodes):\n\n    x_train = training_data_x.reshape(-1, 1)\n    y_train = training_data_y.reshape(-1, 1)\n    \n    # Training loop\n    # Set gradients to zero (pytorch stores gradients for each parameter during backwards pass, they need to be explicitly set to zero before each training iteration)\n    optimizer.zero_grad()\n    # predict target values for the input training data\n    y_pred = regressor(x_train)\n    # compute loss (compare predicted targets to annotations)\n    loss = loss_fn(y_pred, y_train)\n    # perform backward pass -> compute gradients for each parameter)\n    loss.backward()\n    # perform optimizer step, i.e. adjust parameters values according to gradients\n    optimizer.step()\n\n    # store the loss of the performed training iteration (for plotting and tracking of training progress)\n    losses.append(loss.detach().numpy())\n\n    update_viz(fig, axes, losses, training_data_x, training_data_y)"}]}