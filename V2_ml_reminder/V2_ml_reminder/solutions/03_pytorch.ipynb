{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression with PyTorch - Solutions\n",
    "\n",
    "PyTorch is one of the most-used pthon library for deep learning, in a broad variety of applications.\n",
    "\n",
    "Even though it is not designed for simple regression problems, it can be used to solve such tasks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "The following lines import the packages required in this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "import time\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression using PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Creation\n",
    "\n",
    "We explore linear regression in PyTorch using the same problem description as previously in exercise \"01_linear_regression\".\n",
    "\n",
    "You are welcome to copy the code to create training data arrays ``X_train`` and ``y_train``.\n",
    "\n",
    "(Once again, use parameter values ``w0=2.0``, ``w1=1.5`` and add Gaussian noise of standard deviation 0.15 to the y-values.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of input samples\n",
    "n_samples = 15\n",
    "\n",
    "# start solution\n",
    "# x_1 values in the range (0, 10)\n",
    "x1_train = np.random.uniform(0, 10, size=n_samples).reshape(-1, 1)\n",
    "\n",
    "# constants x_0 of the same shape as x_1\n",
    "x0_train = np.ones(shape=x1_train.shape)\n",
    "\n",
    "# input data X\n",
    "X_train = np.concatenate([x0_train, x1_train], axis=1)\n",
    "\n",
    "# true model parameters\n",
    "w0 = 2.0\n",
    "w1 = 1.5\n",
    "parameters = np.array([w0, w1]).reshape(2, 1)\n",
    "\n",
    "# output data y \n",
    "noise = np.random.normal(0, 0.15, size=x1_train.shape)\n",
    "y_train = np.matmul(X_train, parameters) + noise\n",
    "# end solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we convert the training data (x- and y-values) into a PyTorch Tensors, which is necessary for subsequent use with the PyTorch library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_x = torch.Tensor(x1_train) \n",
    "training_data_y = torch.Tensor(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, the linear regression problem will not be solved using the closed form solution, but an iterative loss minimization approach.\n",
    "\n",
    "Even though we have not covered the theory yet (we will do so next week),\n",
    "a simple artificial neuron (without non-linear activation function) is methodically identical to basic linear regression.\n",
    "A simple neural layer is implemented in PyTorch's class ``nn.Linear()``, and a single neuron model can be initialized by ``nn.Linear(1, 1)``.\n",
    "\n",
    "First, implement the class ```LinearRegressor``` which inherits from ```nn.Module```. In the ```__init__``` function add a linear layer using ```nn.Linear()```. This layer takes a single input value x and outputs the corresponding prediction y_pred. Also implement the ```forward(self, x)``` for the forward pass of the regressor. For details see the documentation at https://pytorch.org/docs/stable/generated/torch.nn.Module.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### start solution\n",
    "class LinearRegressor(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y_pred = self.linear(x)\n",
    "        return y_pred\n",
    "### end solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the (parameter-free) constructor of our model class ``LinearRegressor``, the model can be initialized.\n",
    "\n",
    "In this step, model parameters are initialized randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = LinearRegressor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we implement a method to plot the training progress during training using matplotlib and the Jupyter Matplotlib inline feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_viz(fig, axes, losses, training_data_x, training_data_y):\n",
    "    x_viz = torch.linspace(0, 10, 100).reshape(-1, 1)\n",
    "    y_viz = regressor(x_viz).detach().numpy()\n",
    "    axes[0].cla()\n",
    "    axes[1].cla()\n",
    "    axes[0].set_xlim([0, 10])\n",
    "    axes[0].set_ylim([0, 20])\n",
    "    axes[1].set_xlim([0, 100])\n",
    "    axes[1].set_ylim([0, 100])\n",
    "    axes[1].plot(losses, label='mean squared error')\n",
    "    axes[0].scatter(training_data_x.detach().numpy(), training_data_y.detach().numpy(), label='training samples')\n",
    "    axes[0].plot(x_viz, y_viz, color='red', label='current model')\n",
    "    axes[0].legend()\n",
    "    axes[1].legend()\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(plt.gcf())\n",
    "    time.sleep(0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code block performs the training with PyTorch.\n",
    "\n",
    "Using already implemented loss functions and gradients, the computed loss for model prediction's and training data is iteratively minimized, thereby finding the optimal training parameters.\n",
    "\n",
    "During training, the training progress is illustrated below.\n",
    "\n",
    "You can try different learning rates and also vary the training data to see how training process and convergence speed vary.\n",
    "\n",
    "*Attention:* Model parameters are stored in the LinearRegressor instance. If you change the setup, re-initialize the model to reset the training progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "n_episodes = 100\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "learning_rate = 5e-4\n",
    "optimizer = torch.optim.SGD(regressor.parameters(), lr=learning_rate)\n",
    "\n",
    "losses = []\n",
    "for i in range(n_episodes):\n",
    "\n",
    "    x_train = training_data_x.reshape(-1, 1)\n",
    "    y_train = training_data_y.reshape(-1, 1)\n",
    "    \n",
    "    # Training loop\n",
    "    # Set gradients to zero (pytorch stores gradients for each parameter during backwards pass, they need to be explicitly set to zero before each training iteration)\n",
    "    optimizer.zero_grad()\n",
    "    # predict target values for the input training data\n",
    "    y_pred = regressor(x_train)\n",
    "    # compute loss (compare predicted targets to annotations)\n",
    "    loss = loss_fn(y_pred, y_train)\n",
    "    # perform backward pass -> compute gradients for each parameter)\n",
    "    loss.backward()\n",
    "    # perform optimizer step, i.e. adjust parameters values according to gradients\n",
    "    optimizer.step()\n",
    "\n",
    "    # store the loss of the performed training iteration (for plotting and tracking of training progress)\n",
    "    losses.append(loss.detach().numpy())\n",
    "\n",
    "    update_viz(fig, axes, losses, training_data_x, training_data_y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
